% Chapter 2: Literature Review 
\chapter{Literature Review}

\section{Causal Inference Methods in Finance}
Financial economists have developed several quasi-experimental techniques to infer causality from observational data. Two of the most widely used approaches in empirical finance are \textbf{Difference-in-Differences (DiD)} and \textbf{matching methods}. These techniques aim to emulate a randomised controlled experiment using observational data and have been applied in studies of policy changes, market interventions, and factor effects.

\subsection*{Difference-in-Differences and Changes-in-Changes}
DiD is a panel data approach that compares the change in outcomes over time between a \textit{treated} group (affected by some intervention or condition) and a \textit{control} group (not affected). By taking the difference of differences, DiD cancels out time trends common to both groups and any static differences between groups\cite{Athey16}. The key assumption is \textit{parallel trends}: in the absence of treatment, the treated and control groups would have followed the same trajectory. Violations of this assumption (e.g. if the treated group was already on a different trend) distort the estimates. Traditional DiD focuses on average outcomes, providing an estimate of the average treatment effect on the treated.

To capture distributional effects beyond the mean, \textbf{Athey and Imbens (2006)} introduced an extension known as \textit{Changes-in-Changes (CiC)}, which compares the entire outcome distribution before and after treatment by examining quantiles\cite{Athey06}. CiC allows treatment effects to vary across the distribution (heterogeneous effects), relaxing the strict parallel trends assumption. In higher dimensions, analysing full outcome distributions connects naturally to optimal transport methods via concepts like cyclic monotonicity\cite{Gunsilius21}. More recently, \textbf{Torous, Gunsilius, and Rigollet} proposed an OT-based nonlinear DiD framework that explicitly estimates a distributional treatment effect using optimal transport\cite{Torous24}. This approach, which we refer to as "distributional DiD," optimally transports the pre-treatment outcome distribution of the treated group to the post-treatment distribution, adjusting for the control group's changes. It can show how an intervention changes not just the mean of returns but the entire distribution (for instance, whether a policy affects tail risks or volatility of returns, not only the average). In contexts like finance where an intervention might change risk or higher moments of returns, this distribution-level insight is particularly useful\cite{Torous24}.

Moreover, CiC via OT treats the DiD estimation more comprehensively than standard DiD.

\subsection*{Matching and Propensity Scores}
Matching methods attempt to address confounding by pairing each treated unit (e.g. a stock influenced by a factor or event) with one or more similar control units that were not treated. The goal is to create a balanced comparison group that mimics a randomised experiment. A common implementation is \textbf{propensity score matching}, where one first estimates the probability of treatment for each unit (the propensity score, typically via logistic regression on observables) and then matches treated and control units with similar scores\cite{Rosenbaum83}. If the propensity model captures all relevant covariates, matching on this single score should, in theory, balance those covariates between groups.

In practice, matching in high dimensions is challenging. Standard one-to-one nearest neighbor matching can fail if no close counterpart exists for some treated units, and matching on a single-number propensity score may not fully eliminate imbalance on each covariate. \textbf{Optimal Transport} offers a more flexible, distribution-level matching mechanism\cite{Gunsilius21}. Instead of forcing each treated stock to match with one control, OT-based matching finds a transport plan, effectively a weighting of control units, that minimises the overall difference in covariate distributions between treated and control groups. This can involve fractional matches or leaving some observations unmatched. For example, if certain treated stocks have no close counterparts in the control pool, an unbalanced OT algorithm might assign them lower weight or drop them, rather than force a bad match\cite{Gunsilius21}. Gunsilius (2021) discusses how unbalanced OT can improve covariate overlap by allowing some mass to not be matched\cite{Gunsilius21}. In a finance scenario, OT matching could better account for differences in firm size, industry, or other characteristics when assessing a factor's effect by effectively reweighting the control group to mirror the treated group's distribution along those covariates. This reduces bias in estimated treatment effects and improves credibility.

Empirical evidence suggests OT-based matching often yields smaller \textit{standardised mean differences} on covariates (a common balance metric) compared to propensity matching\cite{Gunsilius21}. In this thesis, we will use such metrics to evaluate covariate balance before and after matching. Improved balance is critical: treatment effect estimates (like the Average Treatment Effect on the Treated, ATT) are more reliable when treated and control groups are comparable. If OT can appreciably reduce covariate imbalances, any remaining bias in our factor effect estimates should diminish correspondingly.

\subsection*{OT for Causal Direction (DIVOT)}
Determining the direction of causality between two variables (X causes Y or Y causes X) is a challenging task. \textbf{Tu et al. (2022)} introduced an approach called \textit{DIVOT (Distributional Inference of Variable Order with Transport)} that uses optimal transport in the context of causal discovery\cite{Tu22}. Their method leverages the idea of a \textit{functional causal model} (FCM): if $X \to Y$, then for a given functional relationship plus noise, one can interpret $Y$'s distribution as $X$'s distribution pushed forward through that function. DIVOT computes the OT map from $X$'s distribution to $Y$'s distribution and vice versa, then examines which mapping is more "plausible" under constraints of an FCM (like smoothness or sparsity). Under certain assumptions, only the true causal direction yields a transport map that aligns well with the observed joint distribution\cite{Tu22}. In simple terms, if $X$ causes $Y$, one can transport the distribution of $X$ to match $Y$ with less complexity than the reverse. Tu et al. demonstrated this approach on low-dimensional simulated data and found that it could correctly identify cause-effect pairs that confounded other methods.

In our factor investing scenario, DIVOT could, for example, help clarify whether volatility drives returns or returns drive volatility. There is a long-standing debate: higher volatility might demand a risk premium (volatility $\to$ future returns), or conversely, periods of high returns could reduce volatility through market calm (returns $\to$ volatility). An OT-based causal direction test might offer evidence one way or the other by analysing how distributions of these variables move into each other. While implementing the full DIVOT algorithm is complex, we conceptually include it in our analysis and compare its indications (where available) against a simpler additive noise model approach.

\subsection*{OT for Counterfactuals and Distributional Robustness}
Another relevant related line of work is using OT for counterfactual outcome estimation. \textbf{Charpentier et al. (2023)} show that OT can construct individual-level counterfactuals by transporting each observation in a treated group to an analogous observation in the control group (or vice versa) at the same quantile rank\cite{Charpentier23}. In a finance context, a counterfactual question could be: "What would this stock's return have been if it had not been exposed to the momentum factor as it was?" OT can answer this by finding a "nearest" stock (in distributional terms) in the unexposed group and adjusting for distribution differences\cite{Charpentier23}. The result is a counterfactual return distribution for each stock, from which we can infer how much of its performance was due to the factor exposure. These methods go beyond simple averages, highlighting heterogeneous effects on each unit.

Also, OT provides a tool to evaluate \textit{robustness to distributional shifts}. In finance, regime changes or market shifts can break causal links if models are not robust. By explicitly modelling distribution changes (instead of just assuming stationarity), OT-based methods may be less vulnerable to time-varying relationships between factors and returns, making them more suitable for the factor investing context. For example, if a factor's effect is found under one distribution of covariates, OT could simulate how that effect might change if the market moved to a different state (e.g., higher volatility regime) by transporting the covariate distribution. This thesis focuses on identifying causal effects, but in discussion we will explore how OT-enhanced causal analysis might yield strategies that are more stable across changing distributions.

\section{Recent Developments in Causal Analysis}
Our work is also informed by broader advances in econometrics and causal inference in the social sciences. \textbf{Athey and Imbens (2016)} survey the "state of applied econometrics" and emphasise credible design and validation techniques for causal studies\cite{Athey16}. They highlight recent innovations such as synthetic control methods, refined DiD variants, and rigorous robustness checks (placebo tests, sensitivity analyses) as essential tools for empirical researchers. The message is that finding a statistically significant effect is not enough, one must investigate how sensitive the finding is to assumptions and ensure that identification strategies are sound. We apply this approach by incorporating robustness checks (like placebo tests for our synthetic intervention) to verify that our causal discovery algorithms aren't erroneously detecting effects where none exist.

\textbf{Imbens (2024)} provides another perspective, focusing on the integration of machine learning with causal inference and the challenges/opportunities of big data\cite{Imbens24}. He notes that while classical methods (DiD, IV, matching) have solid theoretical foundations, newer techniques (including those leveraging ML and OT) can handle more complex scenarios and large datasets. In factor investing, the availability of rich datasets (many stocks, high-frequency data, etc.) means methods that can exploit more data and capture nonlinear patterns, without sacrificing identification rigor, are valuable. Imbens also underscores that as datasets grow, traditional concerns (like overfitting or multiple hypothesis testing across the "factor zoo") become important, and combining ML with sound econometrics is a way forward\cite{Imbens24}. Our thesis's use of synthetic data and algorithmic causal discovery can be seen as a modest example of this: we use computational tools to identify causal relations, but we validate them through econometric logic and known ground truth.

Methods inspired by Angrist and Pischke emphasise randomised or natural experiments, matching on observables, control methods, refined DiD variants, and careful robustness checks (placebo tests, sensitivity analyses) as essential tools for empirical researchers. The message is that finding a statistically significant effect is not enough, one must investigate how sensitive the finding is to assumptions and ensure that identification strategies are sound.

In summary, the literature suggests that combining established causal inference frameworks with modern computational techniques (such as OT) shows potential. The identification strategies from econometrics ensure validity, while OT and ML contribute flexibility and depth in analysis. This thesis builds on these ideas, aiming to show that such a combination can indeed move factor investing research toward robust, causally grounded insights. By designing our simulation and choice of methods based on previous research, we ensure that our approach is not only novel but also rooted in the best practices and lessons learned from prior work.