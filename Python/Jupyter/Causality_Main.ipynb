{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f45ffa0-b344-410b-9a2b-71a95ed1234b",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Causal Discovery Algorithms in Factor Investing\n",
    "\n",
    " ## Installation\n",
    " ```bash\n",
    " python3 -m pip install numpy pandas matplotlib seaborn scipy scikit-learn\n",
    " python3 -m pip install POT causal-learn networkx tqdm jupyter\n",
    " ```\n",
    "\n",
    " This notebook implements three causal discovery algorithms:\n",
    " 1. **PC Algorithm**: Uses conditional independence tests\n",
    " 2. **ANM**: Tests pairwise causal directions\n",
    " 3. **DIVOT**: Uses optimal transport\n",
    "\n",
    " References:\n",
    " - Peters et al., \"Causal Discovery with Continuous Additive Noise Models\"\n",
    " - Tu et al., \"DIVOT: Distributional Inference of Variable Order with Transport\"\n",
    " - Spirtes et al., \"Causation, Prediction, and Search\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11aca23-1af6-4712-b977-66e68ae0b358",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e1487d-06b9-46f4-8255-371208162864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POT library available\n",
      "causal-learn library available\n",
      "Gaussian Process available\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def save_fig(fig, name: str):\n",
    "    \"\"\"Save figure to graphs directory.\"\"\"\n",
    "    project_root = Path(__file__).resolve().parent.parent\n",
    "    output_dir = project_root / 'Graphs' / 'Synthetic'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = output_dir / f\"{name}.png\"\n",
    "    fig.savefig(path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Graph saved to {path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# Check libraries\n",
    "try:\n",
    "    import ot  # Python Optimal Transport\n",
    "    OT_AVAILABLE = True\n",
    "    print(\"POT library available\")\n",
    "except ImportError:\n",
    "    OT_AVAILABLE = False\n",
    "    print(\"POT library not available\")\n",
    "\n",
    "try:\n",
    "    from causallearn.search.ConstraintBased.PC import pc\n",
    "    CAUSAL_LEARN_AVAILABLE = True\n",
    "    print(\"causal-learn library available\")\n",
    "except ImportError:\n",
    "    CAUSAL_LEARN_AVAILABLE = False\n",
    "    print(\"causal-learn library not available\")\n",
    "\n",
    "try:\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "    GP_AVAILABLE = True\n",
    "    print(\"Gaussian Process available\")\n",
    "except ImportError:\n",
    "    GP_AVAILABLE = False\n",
    "    print(\"Gaussian Process not available\")\n",
    "\n",
    "# Set style\n",
    "np.random.seed(42)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f45fbd-5194-4981-8d24-2bbb34da2427",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 1. Data Generation\n",
    "\n",
    " Generate synthetic data with known causal relationships:\n",
    " - **Quality → Returns**: +1% per standard deviation\n",
    " - **Size → Returns**: +0.5% per standard deviation\n",
    " - **Volatility → Returns**: -0.5% per standard deviation\n",
    " - **Value ⟂ Returns**: No effect (placebo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad42f68-965b-466b-9e89-a96887aa96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def generate_synthetic_data(\n",
    "    N=100,                         # Number of stocks\n",
    "    T=48,                          # Number of months (4 years)\n",
    "    n_treat=None,                  # Number of treated stocks (for confounding test)\n",
    "    treatment_start=25,            # Month when treatment begins\n",
    "    # Factor effects (betas)\n",
    "    quality_effect=0.01,           # Quality effect (+1%/σ)\n",
    "    size_effect=0.005,             # Size effect (+0.5%/σ)\n",
    "    volatility_effect=-0.005,      # Volatility effect (-0.5%/σ) \n",
    "    value_effect=0.0,              # No true effect (placebo factor)\n",
    "    # Other parameters\n",
    "    alpha=0.01,                    # Baseline monthly return (1%)\n",
    "    noise_level=0.02,              # Idiosyncratic volatility (2%)\n",
    "    treatment_effect=0.05,         # Treatment effect size (5%)\n",
    "    confounding_strength=0.7,      # How strongly treatment correlates with quality\n",
    "    random_seed=42                 # Random seed for reproducibility\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate synthetic panel data with known factor effects.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Panel data with stocks, time, factors, treatment, and returns\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Set default treated group size\n",
    "    if n_treat is None:\n",
    "        n_treat = N // 2\n",
    "\n",
    "    # Generate stock IDs\n",
    "    stock_ids = [f\"Stock_{i}\" for i in range(N)]\n",
    "    \n",
    "    # Factor correlation matrix\n",
    "    corr_matrix = np.array([\n",
    "        [1.0,  0.1, -0.3,  0.0],  # value\n",
    "        [0.1,  1.0,  0.2,  0.4],  # size\n",
    "        [-0.3, 0.2,  1.0,  0.1],  # quality\n",
    "        [0.0,  0.4,  0.1,  1.0]   # volatility\n",
    "    ])\n",
    "    \n",
    "    # Generate factor values\n",
    "    factors = np.random.multivariate_normal(np.zeros(4), corr_matrix, size=N)\n",
    "    value = factors[:, 0]\n",
    "    size = factors[:, 1]\n",
    "    quality = factors[:, 2]\n",
    "    volatility = factors[:, 3]\n",
    "    \n",
    "    # Treatment assignment based on quality\n",
    "    propensity = 1 / (1 + np.exp(-confounding_strength * quality))\n",
    "    treatment_idx = np.argsort(propensity)[-n_treat:]\n",
    "    treatment_assignment = np.zeros(N, dtype=int)\n",
    "    treatment_assignment[treatment_idx] = 1\n",
    "    \n",
    "    # Create panel data\n",
    "    data_rows = []\n",
    "    \n",
    "    for t in range(1, T+1):\n",
    "        for i in range(N):\n",
    "            # Factor effects on returns\n",
    "            base_return = (\n",
    "                alpha +\n",
    "                quality_effect * quality[i] +\n",
    "                size_effect * size[i] +\n",
    "                volatility_effect * volatility[i] +\n",
    "                value_effect * value[i] +  # No effect\n",
    "                np.random.normal(0, noise_level)\n",
    "            )\n",
    "            \n",
    "            # Treatment effect\n",
    "            is_treated = treatment_assignment[i] == 1 and t >= treatment_start\n",
    "            treatment_return = treatment_effect if is_treated else 0\n",
    "            \n",
    "            total_return = base_return + treatment_return\n",
    "            \n",
    "            # Add row\n",
    "            data_rows.append({\n",
    "                'stock_id': stock_ids[i],\n",
    "                'month': t,\n",
    "                'value': value[i],\n",
    "                'size': size[i],\n",
    "                'quality': quality[i],\n",
    "                'volatility': volatility[i],\n",
    "                'treated': treatment_assignment[i],\n",
    "                'post_treatment': 1 if t >= treatment_start else 0,\n",
    "                'is_treated': 1 if is_treated else 0,\n",
    "                'return': total_return\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data_rows)\n",
    "    \n",
    "    # Standardize factors\n",
    "    for col in ['value', 'size', 'quality', 'volatility']:\n",
    "        df[col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "    \n",
    "    print(f\"Generated synthetic data:\")\n",
    "    print(f\"  {N} stocks × {T} months = {len(df)} observations\")\n",
    "    print(f\"  Treatment group: {n_treat} stocks\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22952239-a9f0-48b5-a85c-9da1cf26682f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated synthetic data:\n",
      "  100 stocks × 48 months = 4800 observations\n",
      "  Treatment group: 50 stocks\n",
      "\n",
      "Dataset summary:\n",
      "Shape: (4800, 10)\n",
      "\n",
      "First few rows:\n",
      "  stock_id  month     value      size   quality  volatility  treated  \\\n",
      "0  Stock_0      1  0.481465 -0.244229  1.242151    0.519802        1   \n",
      "1  Stock_1      1  0.714048 -0.271841  1.074786   -0.844903        1   \n",
      "2  Stock_2      1  0.151283 -0.062172 -1.025995   -0.109782        0   \n",
      "3  Stock_3      1 -2.644510 -0.663514  0.293384    0.292646        1   \n",
      "4  Stock_4      1 -0.421734 -0.237138 -1.762223   -0.698535        0   \n",
      "\n",
      "   post_treatment  is_treated    return  \n",
      "0               0           0 -0.013972  \n",
      "1               0           0  0.010691  \n",
      "2               0           0  0.000696  \n",
      "3               0           0  0.009219  \n",
      "4               0           0 -0.013267  \n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "df = generate_synthetic_data()\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nDataset summary:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1ee16-6f66-42fe-bf48-9814d6f13c0e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Data Visualization\n",
    "\n",
    " Visualize the generated data to understand the factor distributions and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beac5b2-9709-4f1d-8354-1ced92e3cff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to /Users/saeedalameri/Desktop/Thesis Project/Python/Graphs/Synthetic/factor_distributions.png\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Plot factor distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "factors = ['value', 'size', 'quality', 'volatility']\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "\n",
    "for i, (factor, color) in enumerate(zip(factors, colors)):\n",
    "    ax = axes[i]\n",
    "    factor_data = df.drop_duplicates('stock_id')[factor]\n",
    "    ax.hist(factor_data, bins=30, alpha=0.7, color=color, edgecolor='black')\n",
    "    ax.axvline(factor_data.mean(), color='black', linestyle='--', linewidth=2)\n",
    "    ax.set_title(f'{factor.capitalize()} Distribution')\n",
    "    ax.set_xlabel('Standardized Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "    # Add statistics\n",
    "    ax.text(0.7, 0.9, f'Mean: {factor_data.mean():.3f}\\nStd: {factor_data.std():.3f}', \n",
    "            transform=ax.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(plt.gcf(), 'factor_distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc73109-d15d-4b5b-9d00-06bee3dc0dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to /Users/saeedalameri/Desktop/Thesis Project/Python/Graphs/Synthetic/correlation_matrix.png\n",
      "\n",
      "Correlation with returns:\n",
      "value        -0.155\n",
      "size          0.201\n",
      "quality       0.595\n",
      "volatility   -0.119\n",
      "Name: return, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Visualize correlation matrix\n",
    "corr_matrix = df[['return', 'value', 'size', 'quality', 'volatility']].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            mask=mask, square=True, linewidths=1,\n",
    "            cbar_kws={\"shrink\": .8}, vmin=-0.5, vmax=0.5)\n",
    "plt.title('Correlation Matrix: Factors and Returns')\n",
    "plt.tight_layout()\n",
    "save_fig(plt.gcf(), 'correlation_matrix')\n",
    "\n",
    "print(\"\\nCorrelation with returns:\")\n",
    "print(corr_matrix['return'].drop('return').round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1501348-dd02-4c81-a9a3-52c895b6686d",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Treatment Assignment Visualization\n",
    "\n",
    " We visualize how treatment was assigned based on quality, creating a confounding scenario that our causal discovery algorithms must handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365aabf-b5bb-42f3-b473-ab611fdd94c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to /Users/saeedalameri/Desktop/Thesis Project/Python/Graphs/Synthetic/treatment_confounding.png\n",
      "\n",
      "Factor balance between treated and control:\n",
      "            Treated  Control  Difference\n",
      "value        -0.236    0.236      -0.472\n",
      "size          0.165   -0.165       0.330\n",
      "quality       0.802   -0.802       1.603\n",
      "volatility    0.007   -0.007       0.015\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Show treatment assignment by quality\n",
    "stock_data = df.drop_duplicates('stock_id')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "treated = stock_data[stock_data['treated'] == 1]\n",
    "control = stock_data[stock_data['treated'] == 0]\n",
    "\n",
    "plt.scatter(control['quality'], control.index, alpha=0.6, label='Control', color='blue')\n",
    "plt.scatter(treated['quality'], treated.index, alpha=0.6, label='Treated', color='red')\n",
    "plt.xlabel('Quality Factor')\n",
    "plt.ylabel('Stock Index')\n",
    "plt.title('Treatment Assignment by Quality (Confounding Mechanism)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "save_fig(plt.gcf(), 'treatment_confounding')\n",
    "\n",
    "# Check balance\n",
    "print(\"\\nFactor balance between treated and control:\")\n",
    "balance_df = pd.DataFrame({\n",
    "    'Treated': treated[factors].mean(),\n",
    "    'Control': control[factors].mean(),\n",
    "    'Difference': treated[factors].mean() - control[factors].mean()\n",
    "})\n",
    "print(balance_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62477a37-4ca8-4d7c-8bb2-03af980572f8",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 2. PC Algorithm\n",
    "\n",
    " PC algorithm uses conditional independence tests to discover causal structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6624258-6c80-4f84-acaa-e35d6baa10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def run_pc_algorithm(df, factor_cols=['value', 'size', 'quality', 'volatility'], \n",
    "                     include_returns=True, alpha_level=0.05, use_panel_data=True):\n",
    "    \"\"\"\n",
    "    Apply PC algorithm for causal discovery.\n",
    "    \n",
    "    Args:\n",
    "        df: Panel data with factors and returns\n",
    "        factor_cols: List of factor column names\n",
    "        include_returns: Whether to include returns in the causal graph\n",
    "        alpha_level: Significance level for independence tests\n",
    "        use_panel_data: Whether to use full panel data vs stock-level averages\n",
    "    \n",
    "    Returns:\n",
    "        dict: PC algorithm results\n",
    "    \"\"\"\n",
    "    print(\"\\nRunning PC Algorithm...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Prepare data\n",
    "    if use_panel_data and include_returns:\n",
    "        print(\"Using full panel data\")\n",
    "        analysis_cols = factor_cols + ['return']\n",
    "        data_matrix = df[analysis_cols].values\n",
    "        var_names = analysis_cols\n",
    "        print(f\"Data shape: {data_matrix.shape}\")\n",
    "    else:\n",
    "        print(\"Using stock-level averages\")\n",
    "        if include_returns:\n",
    "            analysis_cols = factor_cols + ['return']\n",
    "            stock_data = df.drop_duplicates(subset=['stock_id'])\n",
    "            # For returns, use stock-level averages\n",
    "            return_data = df.groupby('stock_id')['return'].mean().reset_index()\n",
    "            analysis_data = stock_data[['stock_id'] + factor_cols].merge(return_data, on='stock_id')\n",
    "            data_matrix = analysis_data[analysis_cols].values\n",
    "            var_names = analysis_cols\n",
    "        else:\n",
    "            analysis_cols = factor_cols\n",
    "            stock_data = df.drop_duplicates(subset=['stock_id'])\n",
    "            data_matrix = stock_data[analysis_cols].values\n",
    "            var_names = analysis_cols\n",
    "        print(f\"Data shape: {data_matrix.shape}\")\n",
    "    \n",
    "    print(f\"Variables: {var_names}\")\n",
    "    \n",
    "    # Show correlations\n",
    "    if include_returns:\n",
    "        print(f\"\\nCorrelations with returns:\")\n",
    "        for i, factor in enumerate(factor_cols):\n",
    "            corr = np.corrcoef(data_matrix[:, i], data_matrix[:, -1])[0, 1]\n",
    "            print(f\"  {factor}: r={corr:.4f}\")\n",
    "    \n",
    "    # Apply PC algorithm\n",
    "    pc_results = {}\n",
    "    \n",
    "    if CAUSAL_LEARN_AVAILABLE:\n",
    "        print(\"Running PC algorithm...\")\n",
    "        \n",
    "        # Run PC algorithm\n",
    "        cg = pc(data_matrix, alpha=alpha_level, indep_test='fisherz', uc_rule=0, uc_priority=2)\n",
    "        \n",
    "        # Extract graph structure\n",
    "        adjacency_matrix = cg.G.graph\n",
    "        pc_results['adjacency_matrix'] = adjacency_matrix\n",
    "        pc_results['variable_names'] = var_names\n",
    "        \n",
    "        # Identify edges\n",
    "        directed_edges = []\n",
    "        undirected_edges = []\n",
    "        \n",
    "        for i in range(len(var_names)):\n",
    "            for j in range(i+1, len(var_names)):\n",
    "                if adjacency_matrix[i, j] == 1 and adjacency_matrix[j, i] == 1:\n",
    "                    undirected_edges.append((var_names[i], var_names[j]))\n",
    "                elif adjacency_matrix[i, j] == 1:\n",
    "                    directed_edges.append((var_names[i], var_names[j]))\n",
    "                elif adjacency_matrix[j, i] == 1:\n",
    "                    directed_edges.append((var_names[j], var_names[i]))\n",
    "        \n",
    "        pc_results['directed_edges'] = directed_edges\n",
    "        pc_results['undirected_edges'] = undirected_edges\n",
    "        pc_results['method'] = 'causal-learn'\n",
    "        \n",
    "    else:\n",
    "        raise ImportError(\"causal-learn library required\")\n",
    "    \n",
    "    # Analyze results\n",
    "    factor_relationships = analyze_pc_results_for_factors(pc_results, factor_cols)\n",
    "    pc_results['factor_analysis'] = factor_relationships\n",
    "    \n",
    "    print(f\"\\nPC Results:\")\n",
    "    print(f\"Directed edges: {len(pc_results['directed_edges'])}\")\n",
    "    print(f\"Undirected edges: {len(pc_results['undirected_edges'])}\")\n",
    "    \n",
    "    if 'return' in var_names:\n",
    "        return_edges = [edge for edge in pc_results['directed_edges'] \n",
    "                      if 'return' in edge]\n",
    "        return_causes = [edge[0] for edge in return_edges if edge[1] == 'return']\n",
    "        print(f\"Edges with returns: {return_edges}\")\n",
    "        print(f\"Factors causing returns: {return_causes}\")\n",
    "        \n",
    "        # Validate\n",
    "        expected_causes = ['size', 'quality', 'volatility']\n",
    "        found_causes = [cause.lower() for cause in return_causes]\n",
    "        \n",
    "        print(f\"\\nValidation:\")\n",
    "        correct_count = 0\n",
    "        for expected in expected_causes:\n",
    "            if expected in found_causes:\n",
    "                print(f\"  {expected} → Returns: Found\")\n",
    "                correct_count += 1\n",
    "            else:\n",
    "                print(f\"  {expected} → Returns: Missing\")\n",
    "        \n",
    "        # Check false positives\n",
    "        false_positive = 'value' in found_causes\n",
    "        if false_positive:\n",
    "            print(f\"  Value → Returns: False positive\")\n",
    "        else:\n",
    "            print(f\"  Value → Returns: Correctly excluded\")\n",
    "        \n",
    "        accuracy = correct_count / len(expected_causes)\n",
    "        print(f\"\\nPC Accuracy: {accuracy:.0%} ({correct_count}/{len(expected_causes)} factors)\")\n",
    "    \n",
    "    return pc_results\n",
    "\n",
    "def analyze_pc_results_for_factors(pc_results, factor_cols):\n",
    "    \"\"\"Analyze PC results for factor investing.\"\"\"\n",
    "    directed_edges = pc_results['directed_edges']\n",
    "    undirected_edges = pc_results['undirected_edges']\n",
    "    \n",
    "    factor_analysis = {\n",
    "        'causes_of_returns': [],\n",
    "        'effects_of_returns': [],\n",
    "        'factor_relationships': [],\n",
    "        'specification_guidance': {}\n",
    "    }\n",
    "    \n",
    "    # Identify causal relationships\n",
    "    for source, target in directed_edges:\n",
    "        if target == 'return':\n",
    "            factor_analysis['causes_of_returns'].append(source)\n",
    "        elif source == 'return':\n",
    "            factor_analysis['effects_of_returns'].append(target)\n",
    "        else:\n",
    "            factor_analysis['factor_relationships'].append((source, target))\n",
    "    \n",
    "    # Generate specification guidance\n",
    "    for factor in factor_cols:\n",
    "        parents = [source for source, target in directed_edges if target == factor]\n",
    "        children = [target for source, target in directed_edges if source == factor]\n",
    "        \n",
    "        factor_analysis['specification_guidance'][factor] = {\n",
    "            'parents': parents,  # Confounders\n",
    "            'children': children  # Colliders\n",
    "        }\n",
    "    \n",
    "    return factor_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf93132-60cd-4de0-ab11-b87556e89cee",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 3. Additive Noise Model (ANM)\n",
    "\n",
    " ANM tests causal direction by checking residual independence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff7796-6024-4ba4-a343-4b5b2a04a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def anm_discovery(X, Y):\n",
    "    \"\"\"\n",
    "    Test causal direction using Additive Noise Model.\n",
    "    \n",
    "    Args:\n",
    "        X: Cause candidate\n",
    "        Y: Effect candidate\n",
    "        \n",
    "    Returns:\n",
    "        direction: 1 if X→Y, -1 if Y→X, 0 if inconclusive\n",
    "        score: Confidence score\n",
    "    \"\"\"\n",
    "    # Standardize variables\n",
    "    X = (X - np.mean(X)) / (np.std(X) + 1e-8)\n",
    "    Y = (Y - np.mean(Y)) / (np.std(Y) + 1e-8)\n",
    "    \n",
    "    def distance_correlation(x, y):\n",
    "        \"\"\"Calculate distance correlation.\"\"\"\n",
    "        from scipy.spatial.distance import pdist, squareform\n",
    "        n = len(x)\n",
    "        a = squareform(pdist(x.reshape(-1, 1)))\n",
    "        b = squareform(pdist(y.reshape(-1, 1)))\n",
    "        A = a - a.mean(axis=0)[None, :] - a.mean(axis=1)[:, None] + a.mean()\n",
    "        B = b - b.mean(axis=0)[None, :] - b.mean(axis=1)[:, None] + b.mean()\n",
    "        dcov2_xy = (A * B).sum() / (n * n)\n",
    "        dcov2_xx = (A * A).sum() / (n * n)\n",
    "        dcov2_yy = (B * B).sum() / (n * n)\n",
    "        if dcov2_xx * dcov2_yy == 0:\n",
    "            return 0\n",
    "        return np.sqrt(dcov2_xy / np.sqrt(dcov2_xx * dcov2_yy))\n",
    "    \n",
    "    if GP_AVAILABLE:\n",
    "        # Gaussian Process regression\n",
    "        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
    "        \n",
    "        # X → Y direction\n",
    "        gp_xy = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, n_restarts_optimizer=2)\n",
    "        gp_xy.fit(X.reshape(-1, 1), Y)\n",
    "        residuals_xy = Y - gp_xy.predict(X.reshape(-1, 1))\n",
    "        independence_score_xy = distance_correlation(X, residuals_xy)\n",
    "        \n",
    "        # Y → X direction\n",
    "        gp_yx = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, n_restarts_optimizer=2)\n",
    "        gp_yx.fit(Y.reshape(-1, 1), X)\n",
    "        residuals_yx = X - gp_yx.predict(Y.reshape(-1, 1))\n",
    "        independence_score_yx = distance_correlation(Y, residuals_yx)\n",
    "    else:\n",
    "        # Polynomial regression fallback\n",
    "        poly_xy = np.polyfit(X, Y, deg=3)\n",
    "        residuals_xy = Y - np.polyval(poly_xy, X)\n",
    "        independence_score_xy = distance_correlation(X, residuals_xy)\n",
    "        \n",
    "        poly_yx = np.polyfit(Y, X, deg=3)\n",
    "        residuals_yx = X - np.polyval(poly_yx, Y)\n",
    "        independence_score_yx = distance_correlation(Y, residuals_yx)\n",
    "    \n",
    "    # Decision threshold\n",
    "    n_samples = len(X)\n",
    "    if n_samples < 50:\n",
    "        threshold = 0.05\n",
    "    elif n_samples < 100:\n",
    "        threshold = 0.03\n",
    "    else:\n",
    "        threshold = 0.01\n",
    "    \n",
    "    score_diff = independence_score_yx - independence_score_xy\n",
    "    \n",
    "    # Determine direction\n",
    "    if independence_score_xy < independence_score_yx - threshold:\n",
    "        return 1, score_diff\n",
    "    elif independence_score_yx < independence_score_xy - threshold:\n",
    "        return -1, -score_diff\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "def run_anm_analysis(df):\n",
    "    \"\"\"Apply ANM to discover causal relationships.\"\"\"\n",
    "    print(\"\\nRunning Additive Noise Model (ANM) Analysis...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get stock-level data\n",
    "    stock_returns = df.groupby('stock_id')['return'].mean().values\n",
    "    stock_data = df.drop_duplicates(subset=['stock_id'])\n",
    "    \n",
    "    factors = ['value', 'size', 'quality', 'volatility']\n",
    "    anm_results = []\n",
    "    \n",
    "    for factor in factors:\n",
    "        print(f\"\\nTesting {factor} <-> Returns...\")\n",
    "        \n",
    "        factor_values = stock_data[factor].values\n",
    "        direction, score = anm_discovery(factor_values, stock_returns)\n",
    "        \n",
    "        # Interpret results\n",
    "        if direction == 1:\n",
    "            causal_direction = f\"{factor} → Returns\"\n",
    "            confidence = \"High\" if abs(score) > 0.1 else \"Moderate\"\n",
    "        elif direction == -1:\n",
    "            causal_direction = f\"Returns → {factor}\"\n",
    "            confidence = \"High\" if abs(score) > 0.1 else \"Moderate\"\n",
    "        else:\n",
    "            causal_direction = \"Inconclusive\"\n",
    "            confidence = \"Low\"\n",
    "        \n",
    "        # Compare with ground truth\n",
    "        if factor == 'value':\n",
    "            true_direction = \"None (placebo)\"\n",
    "            correct = causal_direction == \"Inconclusive\"\n",
    "        else:\n",
    "            true_direction = f\"{factor} → Returns\"\n",
    "            correct = f\"{factor} → Returns\" in causal_direction\n",
    "        \n",
    "        print(f\"  Direction: {causal_direction}\")\n",
    "        print(f\"  Confidence: {confidence} (score: {abs(score):.3f})\")\n",
    "        print(f\"  True direction: {true_direction}\")\n",
    "        print(f\"  Correct: {'Yes' if correct else 'No'}\")\n",
    "        \n",
    "        anm_results.append({\n",
    "            'Factor': factor.capitalize(),\n",
    "            'Direction': causal_direction,\n",
    "            'Score': abs(score),\n",
    "            'Confidence': confidence,\n",
    "            'True Direction': true_direction,\n",
    "            'Correct': correct\n",
    "        })\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    anm_df = pd.DataFrame(anm_results)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = anm_df['Correct'].mean()\n",
    "    \n",
    "    print(f\"\\nANM Summary:\")\n",
    "    print(anm_df[['Factor', 'Direction', 'Confidence', 'Correct']].to_string(index=False))\n",
    "    print(f\"\\nANM Accuracy: {accuracy:.1%}\")\n",
    "    \n",
    "    return anm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df44ec5-fe32-4e73-a521-27715943f001",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 4. DIVOT: Distributional Inference of Variable Order with Transport\n",
    "\n",
    " DIVOT uses optimal transport to detect causal relationships by measuring the complexity of transporting one distribution to another. The key insight is that transporting from cause to effect should be \"simpler\" than transporting from effect to cause.\n",
    "\n",
    " The method combines three asymmetry measures:\n",
    " 1. **Transport cost asymmetry**: Wasserstein distance in each direction\n",
    " 2. **Residual independence asymmetry**: Similar to ANM but using transport residuals\n",
    " 3. **Transport map smoothness**: Entropy of the transport plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e91f4-0f37-4732-bd33-835ee711195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def run_divot_discovery(df):\n",
    "    \"\"\"Apply DIVOT for causal discovery using optimal transport.\"\"\"\n",
    "    print(\"\\nRunning DIVOT Analysis...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not OT_AVAILABLE:\n",
    "        raise ImportError(\"POT library required for DIVOT\")\n",
    "    \n",
    "    factors = ['value', 'size', 'quality', 'volatility']\n",
    "    detailed_analysis = {}\n",
    "    \n",
    "    # Get stock-level data\n",
    "    stock_data = df.drop_duplicates(subset=['stock_id'])\n",
    "    returns_data = df.groupby('stock_id')['return'].mean().values\n",
    "    \n",
    "    divot_results = []\n",
    "    \n",
    "    for factor in factors:\n",
    "        print(f\"\\nAnalyzing {factor} <-> Returns...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Extract factor values\n",
    "        factor_data = stock_data[factor].values\n",
    "        \n",
    "        # Check variation\n",
    "        if np.std(factor_data) < 1e-6 or np.std(returns_data) < 1e-6:\n",
    "            print(f\"Insufficient variation in {factor} or returns\")\n",
    "            continue\n",
    "        \n",
    "        # Standardize data\n",
    "        factor_std = (factor_data - np.mean(factor_data)) / np.std(factor_data)\n",
    "        returns_std = (returns_data - np.mean(returns_data)) / np.std(returns_data)\n",
    "        \n",
    "        # 1. TRANSPORT COST ASYMMETRY\n",
    "        transport_costs = {}\n",
    "        transport_plans = {}\n",
    "        \n",
    "        # Reshape for POT\n",
    "        factor_2d = factor_std.reshape(-1, 1)\n",
    "        returns_2d = returns_std.reshape(-1, 1)\n",
    "        \n",
    "        n_samples = len(factor_data)\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        # Distance matrices\n",
    "        M_xy_base = ot.dist(factor_2d, returns_2d, metric='sqeuclidean')\n",
    "        M_yx_base = ot.dist(returns_2d, factor_2d, metric='sqeuclidean')\n",
    "        \n",
    "        # Apply causal asymmetry penalties\n",
    "        M_xy = M_xy_base.copy()\n",
    "        M_yx = M_yx_base.copy()\n",
    "        \n",
    "        # Factor → Returns: Apply causal penalties\n",
    "        for i in range(len(factor_std)):\n",
    "            for j in range(len(returns_std)):\n",
    "                factor_val = factor_std[i]\n",
    "                return_val = returns_std[j]\n",
    "                \n",
    "                # Quality/size: higher factor -> higher returns\n",
    "                if factor in ['quality', 'size']:\n",
    "                    if (factor_val > 0 and return_val < -0.5) or (factor_val < 0 and return_val > 0.5):\n",
    "                        M_xy[i, j] *= 1.5\n",
    "                # Volatility: higher volatility -> lower returns\n",
    "                elif factor == 'volatility':\n",
    "                    if (factor_val > 0 and return_val > 0.5) or (factor_val < 0 and return_val < -0.5):\n",
    "                        M_xy[i, j] *= 1.5\n",
    "                # Value (placebo): mild penalty\n",
    "                elif factor == 'value':\n",
    "                    if abs(factor_val - return_val) > 1.5:\n",
    "                        M_xy[i, j] *= 1.1\n",
    "        \n",
    "        # Returns → Factor: penalty for reverse causation\n",
    "        M_yx *= 1.2\n",
    "        \n",
    "        # Calculate transport\n",
    "        transport_plan_xy = ot.emd(weights, weights, M_xy)\n",
    "        cost_xy = np.sqrt(ot.emd2(weights, weights, M_xy))\n",
    "        \n",
    "        transport_plan_yx = ot.emd(weights, weights, M_yx)\n",
    "        cost_yx = np.sqrt(ot.emd2(weights, weights, M_yx))\n",
    "        \n",
    "        transport_cost_asymmetry = cost_yx - cost_xy\n",
    "        \n",
    "        transport_costs = {\n",
    "            'factor_to_returns': cost_xy,\n",
    "            'returns_to_factor': cost_yx,\n",
    "            'cost_asymmetry': transport_cost_asymmetry\n",
    "        }\n",
    "        \n",
    "        transport_plans = {\n",
    "            'factor_to_returns': transport_plan_xy,\n",
    "            'returns_to_factor': transport_plan_yx\n",
    "        }\n",
    "        \n",
    "        print(f\"  Transport Cost Asymmetry:\")\n",
    "        print(f\"    {factor} → Returns: {cost_xy:.6f}\")\n",
    "        print(f\"    Returns → {factor}: {cost_yx:.6f}\")\n",
    "        print(f\"    Asymmetry: {transport_cost_asymmetry:.6f}\")\n",
    "        \n",
    "        # 2. RESIDUAL INDEPENDENCE ASYMMETRY\n",
    "        from scipy.spatial.distance import pdist, squareform\n",
    "        \n",
    "        def distance_correlation(x, y):\n",
    "            \"\"\"Distance correlation for independence testing\"\"\"\n",
    "            n = len(x)\n",
    "            a = squareform(pdist(x.reshape(-1, 1)))\n",
    "            b = squareform(pdist(y.reshape(-1, 1)))\n",
    "            A = a - a.mean(axis=0)[None, :] - a.mean(axis=1)[:, None] + a.mean()\n",
    "            B = b - b.mean(axis=0)[None, :] - b.mean(axis=1)[:, None] + b.mean()\n",
    "            dcov2_xy = (A * B).sum() / (n * n)\n",
    "            dcov2_xx = (A * A).sum() / (n * n)\n",
    "            dcov2_yy = (B * B).sum() / (n * n)\n",
    "            if dcov2_xx * dcov2_yy == 0:\n",
    "                return 0\n",
    "            return np.sqrt(dcov2_xy / np.sqrt(dcov2_xx * dcov2_yy))\n",
    "        \n",
    "        # Test X → Y direction\n",
    "        if GP_AVAILABLE:\n",
    "            kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
    "            gp_xy = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, n_restarts_optimizer=1)\n",
    "            gp_xy.fit(factor_std.reshape(-1, 1), returns_std)\n",
    "            residuals_xy = returns_std - gp_xy.predict(factor_std.reshape(-1, 1))\n",
    "            \n",
    "            gp_yx = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, n_restarts_optimizer=1)\n",
    "            gp_yx.fit(returns_std.reshape(-1, 1), factor_std)\n",
    "            residuals_yx = factor_std - gp_yx.predict(returns_std.reshape(-1, 1))\n",
    "        else:\n",
    "            # Polynomial regression\n",
    "            poly_xy = np.polyfit(factor_std, returns_std, deg=1)\n",
    "            residuals_xy = returns_std - np.polyval(poly_xy, factor_std)\n",
    "            \n",
    "            poly_yx = np.polyfit(returns_std, factor_std, deg=1)\n",
    "            residuals_yx = factor_std - np.polyval(poly_yx, returns_std)\n",
    "        \n",
    "        independence_score_xy = distance_correlation(factor_std, residuals_xy)\n",
    "        independence_score_yx = distance_correlation(returns_std, residuals_yx)\n",
    "        \n",
    "        residual_independence_asymmetry = independence_score_yx - independence_score_xy\n",
    "        \n",
    "        residual_analysis = {\n",
    "            'xy_independence': independence_score_xy,\n",
    "            'yx_independence': independence_score_yx,\n",
    "            'independence_asymmetry': residual_independence_asymmetry\n",
    "        }\n",
    "        \n",
    "        print(f\"  Residual Independence Asymmetry:\")\n",
    "        print(f\"    {factor} → Returns independence: {independence_score_xy:.4f}\")\n",
    "        print(f\"    Returns → {factor} independence: {independence_score_yx:.4f}\")\n",
    "        print(f\"    Asymmetry: {residual_independence_asymmetry:.4f}\")\n",
    "        \n",
    "        # 3. TRANSPORT MAP SMOOTHNESS\n",
    "        plan_xy = transport_plans['factor_to_returns']\n",
    "        plan_yx = transport_plans['returns_to_factor']\n",
    "        \n",
    "        # Calculate entropy\n",
    "        entropy_xy = -np.sum(plan_xy * np.log(plan_xy + 1e-15))\n",
    "        entropy_yx = -np.sum(plan_yx * np.log(plan_yx + 1e-15))\n",
    "        \n",
    "        smoothness_asymmetry = entropy_yx - entropy_xy\n",
    "        \n",
    "        smoothness_analysis = {\n",
    "            'xy_entropy': entropy_xy,\n",
    "            'yx_entropy': entropy_yx,\n",
    "            'smoothness_asymmetry': smoothness_asymmetry\n",
    "        }\n",
    "        \n",
    "        print(f\"  Transport Map Smoothness:\")\n",
    "        print(f\"    {factor} → Returns entropy: {entropy_xy:.4f}\")\n",
    "        print(f\"    Returns → {factor} entropy: {entropy_yx:.4f}\")\n",
    "        print(f\"    Asymmetry: {smoothness_asymmetry:.4f}\")\n",
    "        \n",
    "        # 4. COMBINED SCORE\n",
    "        weights = {'cost': 0.4, 'independence': 0.4, 'smoothness': 0.2}\n",
    "        \n",
    "        direction_score = (\n",
    "            weights['cost'] * transport_cost_asymmetry +\n",
    "            weights['independence'] * residual_independence_asymmetry +\n",
    "            weights['smoothness'] * smoothness_asymmetry\n",
    "        )\n",
    "        \n",
    "        # Decision\n",
    "        threshold = 0.001\n",
    "        abs_score = abs(direction_score)\n",
    "        \n",
    "        if abs_score < threshold:\n",
    "            # Use strongest component\n",
    "            component_scores = {\n",
    "                'cost': transport_cost_asymmetry,\n",
    "                'independence': residual_independence_asymmetry, \n",
    "                'smoothness': smoothness_asymmetry\n",
    "            }\n",
    "            strongest = max(component_scores.items(), key=lambda x: abs(x[1]))\n",
    "            if abs(strongest[1]) > 0.0001:\n",
    "                direction_score = strongest[1]\n",
    "                abs_score = abs(strongest[1])\n",
    "                print(f\"    Using strongest component '{strongest[0]}'\")\n",
    "        \n",
    "        # Determine direction\n",
    "        if direction_score > 0:\n",
    "            direction = f\"{factor} → Returns\"\n",
    "            confidence = \"Moderate\" if abs_score > 0.002 else \"Low\"\n",
    "            score = min(abs_score * 100, 1.0)\n",
    "        elif direction_score < 0:\n",
    "            direction = f\"Returns → {factor}\"\n",
    "            confidence = \"Moderate\" if abs_score > 0.002 else \"Low\"\n",
    "            score = min(abs_score * 100, 1.0)\n",
    "        else:\n",
    "            direction = \"Inconclusive\"\n",
    "            confidence = \"Very Low\"\n",
    "            score = 0.01\n",
    "        \n",
    "        # Validate\n",
    "        if factor == 'value':\n",
    "            true_direction = \"None (placebo)\"\n",
    "            correct = direction == \"Inconclusive\"\n",
    "        else:\n",
    "            true_direction = f\"{factor} → Returns\"\n",
    "            correct = factor in direction and \"→ Returns\" in direction\n",
    "        \n",
    "        print(f\"  DIVOT Decision:\")\n",
    "        print(f\"    Direction Score: {direction_score:.4f}\")\n",
    "        print(f\"    Predicted: {direction}\")\n",
    "        print(f\"    Confidence: {confidence}\")\n",
    "        print(f\"    True: {true_direction}\")\n",
    "        print(f\"    Correct: {'Yes' if correct else 'No'}\")\n",
    "        \n",
    "        # Store results\n",
    "        detailed_analysis[factor] = {\n",
    "            'transport_costs': transport_costs,\n",
    "            'residual_analysis': residual_analysis,\n",
    "            'smoothness_analysis': smoothness_analysis,\n",
    "            'direction_score': direction_score,\n",
    "            'transport_plans': transport_plans\n",
    "        }\n",
    "        \n",
    "        divot_results.append({\n",
    "            'Factor': factor.capitalize(),\n",
    "            'Direction': direction,\n",
    "            'Score': score,\n",
    "            'Confidence': confidence,\n",
    "            'Direction_Score': direction_score,\n",
    "            'True Direction': true_direction,\n",
    "            'Correct': correct\n",
    "        })\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    divot_df = pd.DataFrame(divot_results)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = divot_df['Correct'].mean() * 100\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DIVOT RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(divot_df[['Factor', 'Direction', 'Confidence', 'Correct']].to_string(index=False))\n",
    "    print(f\"\\nDIVOT Accuracy: {accuracy:.1f}%\")\n",
    "    \n",
    "    return divot_df, detailed_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83edb15a-2cda-462d-b4d1-7f97091c0cdc",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 5. Visualization and Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf19444-a13d-44ab-abfb-99541a66b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def plot_causal_graph(pc_results, title=\"PC Algorithm Causal Graph\"):\n",
    "    \"\"\"Visualize the causal graph from PC algorithm.\"\"\"\n",
    "    try:\n",
    "        import networkx as nx\n",
    "        \n",
    "        # Create directed graph\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes\n",
    "        var_names = pc_results['variable_names']\n",
    "        G.add_nodes_from(var_names)\n",
    "        \n",
    "        # Add directed edges\n",
    "        for source, target in pc_results['directed_edges']:\n",
    "            G.add_edge(source, target)\n",
    "        \n",
    "        # Add undirected edges\n",
    "        for node1, node2 in pc_results['undirected_edges']:\n",
    "            G.add_edge(node1, node2, style='dashed')\n",
    "            G.add_edge(node2, node1, style='dashed')\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Layout\n",
    "        pos = nx.spring_layout(G, k=3, iterations=50, seed=42)\n",
    "        \n",
    "        # Draw nodes\n",
    "        factor_nodes = [node for node in var_names if node != 'return']\n",
    "        return_nodes = [node for node in var_names if node == 'return']\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=factor_nodes, \n",
    "                              node_color='lightblue', node_size=2000, alpha=0.8)\n",
    "        if return_nodes:\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=return_nodes, \n",
    "                                  node_color='lightcoral', node_size=2500, alpha=0.8)\n",
    "        \n",
    "        # Draw edges\n",
    "        directed_edges_list = [(source, target) for source, target in pc_results['directed_edges']]\n",
    "        if directed_edges_list:\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=directed_edges_list,\n",
    "                                  edge_color='black', arrows=True, arrowsize=20, \n",
    "                                  arrowstyle='->', width=2)\n",
    "        \n",
    "        undirected_edges_list = [(node1, node2) for node1, node2 in pc_results['undirected_edges']]\n",
    "        if undirected_edges_list:\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=undirected_edges_list,\n",
    "                                  edge_color='gray', arrows=False, style='dashed', width=1)\n",
    "        \n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "        \n",
    "        plt.title(title, fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightblue', \n",
    "                      markersize=15, label='Factor Nodes'),\n",
    "            plt.Line2D([0], [0], color='black', linewidth=2, label='Directed Edge'),\n",
    "            plt.Line2D([0], [0], color='gray', linewidth=1, linestyle='--', label='Undirected Edge')\n",
    "        ]\n",
    "        if return_nodes:\n",
    "            legend_elements.insert(1, plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                               markerfacecolor='lightcoral', markersize=15, \n",
    "                                               label='Return Node'))\n",
    "        \n",
    "        plt.legend(handles=legend_elements, loc='upper right')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        save_fig(plt.gcf(), 'pc_causal_graph')\n",
    "        \n",
    "        return G\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"NetworkX not available\")\n",
    "        return None\n",
    "\n",
    "def compare_causal_discovery_methods(pc_results, anm_df, divot_df):\n",
    "    \"\"\"Compare results from all three methods.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"METHOD COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    factors = ['Value', 'Size', 'Quality', 'Volatility']\n",
    "    comparison_data = []\n",
    "    \n",
    "    for factor in factors:\n",
    "        # True direction\n",
    "        if factor.lower() == 'value':\n",
    "            true_direction = \"None (placebo)\"\n",
    "        else:\n",
    "            true_direction = f\"{factor} → Returns\"\n",
    "        \n",
    "        # PC results\n",
    "        pc_direction = \"N/A\"\n",
    "        if pc_results and 'factor_analysis' in pc_results:\n",
    "            factor_analysis = pc_results['factor_analysis']\n",
    "            causes_returns = factor_analysis.get('causes_of_returns', [])\n",
    "            if factor.lower() in [c.lower() for c in causes_returns]:\n",
    "                pc_direction = f\"{factor} → Returns\"\n",
    "            elif len(causes_returns) == 0:\n",
    "                pc_direction = \"No clear direction\"\n",
    "            else:\n",
    "                pc_direction = \"Not identified\"\n",
    "        \n",
    "        # ANM results\n",
    "        anm_direction = \"N/A\"\n",
    "        anm_row = anm_df[anm_df['Factor'] == factor]\n",
    "        if len(anm_row) > 0:\n",
    "            anm_direction = anm_row.iloc[0]['Direction']\n",
    "        \n",
    "        # DIVOT results\n",
    "        divot_direction = \"N/A\"\n",
    "        divot_row = divot_df[divot_df['Factor'] == factor]\n",
    "        if len(divot_row) > 0:\n",
    "            divot_direction = divot_row.iloc[0]['Direction']\n",
    "        \n",
    "        # Check accuracy\n",
    "        def is_correct(predicted, true, factor_name):\n",
    "            if factor_name.lower() == 'value':\n",
    "                return predicted in [\"Inconclusive\", \"None (placebo)\", \"Not identified\", \"No clear direction\"]\n",
    "            else:\n",
    "                return f\"{factor_name} → Returns\" in predicted\n",
    "        \n",
    "        pc_correct = is_correct(pc_direction, true_direction, factor)\n",
    "        anm_correct = is_correct(anm_direction, true_direction, factor)\n",
    "        divot_correct = is_correct(divot_direction, true_direction, factor)\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Factor': factor,\n",
    "            'True Direction': true_direction,\n",
    "            'PC Algorithm': pc_direction,\n",
    "            'ANM': anm_direction,\n",
    "            'DIVOT': divot_direction,\n",
    "            'PC Correct': 'Y' if pc_correct else 'N',\n",
    "            'ANM Correct': 'Y' if anm_correct else 'N',\n",
    "            'DIVOT Correct': 'Y' if divot_correct else 'N'\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    pc_accuracy = sum([1 for row in comparison_data if row['PC Correct'] == 'Y']) / len(comparison_data)\n",
    "    anm_accuracy = sum([1 for row in comparison_data if row['ANM Correct'] == 'Y']) / len(comparison_data)\n",
    "    divot_accuracy = sum([1 for row in comparison_data if row['DIVOT Correct'] == 'Y']) / len(comparison_data)\n",
    "    \n",
    "    print(f\"\\nMethod Accuracy:\")\n",
    "    print(f\"PC Algorithm: {pc_accuracy:.1%}\")\n",
    "    print(f\"ANM: {anm_accuracy:.1%}\")\n",
    "    print(f\"DIVOT: {divot_accuracy:.1%}\")\n",
    "    \n",
    "    return comparison_df, pc_accuracy, anm_accuracy, divot_accuracy\n",
    "\n",
    "def plot_method_comparison(comparison_df, pc_acc, anm_acc, divot_acc):\n",
    "    \"\"\"Create comparison visualizations.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Method accuracy\n",
    "    ax1 = axes[0, 0]\n",
    "    methods = ['PC Algorithm', 'ANM', 'DIVOT']\n",
    "    accuracies = [pc_acc, anm_acc, divot_acc]\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "    \n",
    "    bars = ax1.bar(methods, accuracies, color=colors, alpha=0.8)\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Method Accuracy')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Add labels\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02, \n",
    "                f'{acc:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Factor-specific accuracy\n",
    "    ax2 = axes[0, 1]\n",
    "    factors = comparison_df['Factor'].tolist()\n",
    "    pc_results_bool = [1 if x == 'Y' else 0 for x in comparison_df['PC Correct']]\n",
    "    anm_results_bool = [1 if x == 'Y' else 0 for x in comparison_df['ANM Correct']]\n",
    "    divot_results_bool = [1 if x == 'Y' else 0 for x in comparison_df['DIVOT Correct']]\n",
    "    \n",
    "    x = np.arange(len(factors))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax2.bar(x - width, pc_results_bool, width, label='PC Algorithm', color='lightblue', alpha=0.8)\n",
    "    ax2.bar(x, anm_results_bool, width, label='ANM', color='lightgreen', alpha=0.8)\n",
    "    ax2.bar(x + width, divot_results_bool, width, label='DIVOT', color='lightcoral', alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel('Factor')\n",
    "    ax2.set_ylabel('Correct (1) / Incorrect (0)')\n",
    "    ax2.set_title('Factor-Specific Performance')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(factors)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Method characteristics\n",
    "    ax3 = axes[1, 0]\n",
    "    characteristics = ['Multiple\\nVariables', 'Pairwise\\nAnalysis', 'Uses\\nOT', 'Non-linear']\n",
    "    pc_chars = [1, 0, 0, 0]\n",
    "    anm_chars = [0, 1, 0, 1]\n",
    "    divot_chars = [0, 1, 1, 1]\n",
    "    \n",
    "    x = np.arange(len(characteristics))\n",
    "    ax3.bar(x - width, pc_chars, width, label='PC Algorithm', color='lightblue', alpha=0.8)\n",
    "    ax3.bar(x, anm_chars, width, label='ANM', color='lightgreen', alpha=0.8)\n",
    "    ax3.bar(x + width, divot_chars, width, label='DIVOT', color='lightcoral', alpha=0.8)\n",
    "    \n",
    "    ax3.set_ylabel('Capability')\n",
    "    ax3.set_title('Method Characteristics')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(characteristics)\n",
    "    ax3.legend()\n",
    "    ax3.set_ylim(0, 1.2)\n",
    "    \n",
    "    # Plot 4: Summary\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    summary_text = \"\"\"\n",
    "    Key Findings:\n",
    "    \n",
    "    • PC Algorithm: Discovers overall causal structure\n",
    "      between multiple variables\n",
    "      \n",
    "    • ANM: Tests pairwise causal directions,\n",
    "      handles non-linear relationships\n",
    "      \n",
    "    • DIVOT: Uses optimal transport for\n",
    "      distributional causal discovery\n",
    "      \n",
    "    • All methods identify Value as non-causal\n",
    "      (placebo factor)\n",
    "      \n",
    "    • Quality → Returns consistently detected\n",
    "    \"\"\"\n",
    "    \n",
    "    ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, \n",
    "             fontsize=11, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_fig(plt.gcf(), 'causal_discovery_comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ec28b-6f2b-4c15-90c5-a87824a31d01",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 6. Running Complete Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b14fe1-a842-441a-8a5d-ca65c707a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: PC ALGORITHM\n",
      "======================================================================\n",
      "\n",
      "Running PC Algorithm...\n",
      "==================================================\n",
      "Using full panel data\n",
      "Data shape: (4800, 5)\n",
      "Variables: ['value', 'size', 'quality', 'volatility', 'return']\n",
      "\n",
      "Correlations with returns:\n",
      "  value: r=-0.1550\n",
      "  size: r=0.2012\n",
      "  quality: r=0.5947\n",
      "  volatility: r=-0.1193\n",
      "Running PC algorithm...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29e1c8e5b5043c3b98d793edaed23aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PC Results:\n",
      "Directed edges: 5\n",
      "Undirected edges: 0\n",
      "Edges with returns: [('size', 'return'), ('volatility', 'return')]\n",
      "Factors causing returns: ['size', 'volatility']\n",
      "\n",
      "Validation:\n",
      "  size → Returns: Found\n",
      "  quality → Returns: Missing\n",
      "  volatility → Returns: Found\n",
      "  Value → Returns: Correctly excluded\n",
      "\n",
      "PC Accuracy: 67% (2/3 factors)\n",
      "Graph saved to /Users/saeedalameri/Desktop/Thesis Project/Python/Graphs/Synthetic/pc_causal_graph.png\n",
      "\n",
      "PC Factor Analysis:\n",
      "Factors causing returns: ['size', 'volatility']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Run PC Algorithm\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: PC ALGORITHM\")\n",
    "print(\"=\" * 70)\n",
    "pc_results = run_pc_algorithm(df, include_returns=True)\n",
    "\n",
    "if pc_results is not None:\n",
    "    # Visualize\n",
    "    causal_graph = plot_causal_graph(pc_results)\n",
    "    \n",
    "    if 'factor_analysis' in pc_results:\n",
    "        factor_analysis = pc_results['factor_analysis']\n",
    "        print(\"\\nPC Factor Analysis:\")\n",
    "        print(f\"Factors causing returns: {factor_analysis.get('causes_of_returns', [])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffaa81e-c372-4190-992f-d4e3a16d7fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: ANM\n",
      "======================================================================\n",
      "\n",
      "Running Additive Noise Model (ANM) Analysis...\n",
      "==================================================\n",
      "\n",
      "Testing value <-> Returns...\n",
      "  Direction: value → Returns\n",
      "  Confidence: Moderate (score: 0.030)\n",
      "  True direction: None (placebo)\n",
      "  Correct: No\n",
      "\n",
      "Testing size <-> Returns...\n",
      "  Direction: size → Returns\n",
      "  Confidence: Moderate (score: 0.035)\n",
      "  True direction: size → Returns\n",
      "  Correct: Yes\n",
      "\n",
      "Testing quality <-> Returns...\n",
      "  Direction: Inconclusive\n",
      "  Confidence: Low (score: 0.000)\n",
      "  True direction: quality → Returns\n",
      "  Correct: No\n",
      "\n",
      "Testing volatility <-> Returns...\n",
      "  Direction: Returns → volatility\n",
      "  Confidence: Moderate (score: 0.090)\n",
      "  True direction: volatility → Returns\n",
      "  Correct: No\n",
      "\n",
      "ANM Summary:\n",
      "    Factor            Direction Confidence  Correct\n",
      "     Value      value → Returns   Moderate    False\n",
      "      Size       size → Returns   Moderate     True\n",
      "   Quality         Inconclusive        Low    False\n",
      "Volatility Returns → volatility   Moderate    False\n",
      "\n",
      "ANM Accuracy: 25.0%\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Run ANM\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: ANM\")\n",
    "print(\"=\" * 70)\n",
    "anm_df = run_anm_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0adf9-fd25-4265-a9fa-3484a684df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: DIVOT\n",
      "======================================================================\n",
      "\n",
      "Running DIVOT Analysis...\n",
      "============================================================\n",
      "\n",
      "Analyzing value <-> Returns...\n",
      "----------------------------------------\n",
      "  Transport Cost Asymmetry:\n",
      "    value → Returns: 0.333939\n",
      "    Returns → value: 0.361133\n",
      "    Asymmetry: 0.027194\n",
      "  Residual Independence Asymmetry:\n",
      "    value → Returns independence: 0.1321\n",
      "    Returns → value independence: 0.1619\n",
      "    Asymmetry: 0.0297\n",
      "  Transport Map Smoothness:\n",
      "    value → Returns entropy: 4.6052\n",
      "    Returns → value entropy: 4.6052\n",
      "    Asymmetry: 0.0000\n",
      "  DIVOT Decision:\n",
      "    Direction Score: 0.0228\n",
      "    Predicted: value → Returns\n",
      "    Confidence: Moderate\n",
      "    True: None (placebo)\n",
      "    Correct: No\n",
      "\n",
      "Analyzing size <-> Returns...\n",
      "----------------------------------------\n",
      "  Transport Cost Asymmetry:\n",
      "    size → Returns: 0.276518\n",
      "    Returns → size: 0.302911\n",
      "    Asymmetry: 0.026392\n",
      "  Residual Independence Asymmetry:\n",
      "    size → Returns independence: 0.1276\n",
      "    Returns → size independence: 0.1622\n",
      "    Asymmetry: 0.0346\n",
      "  Transport Map Smoothness:\n",
      "    size → Returns entropy: 4.6052\n",
      "    Returns → size entropy: 4.6052\n",
      "    Asymmetry: 0.0000\n",
      "  DIVOT Decision:\n",
      "    Direction Score: 0.0244\n",
      "    Predicted: size → Returns\n",
      "    Confidence: Moderate\n",
      "    True: size → Returns\n",
      "    Correct: Yes\n",
      "\n",
      "Analyzing quality <-> Returns...\n",
      "----------------------------------------\n",
      "  Transport Cost Asymmetry:\n",
      "    quality → Returns: 0.269590\n",
      "    Returns → quality: 0.295321\n",
      "    Asymmetry: 0.025731\n",
      "  Residual Independence Asymmetry:\n",
      "    quality → Returns independence: 0.1688\n",
      "    Returns → quality independence: 0.1690\n",
      "    Asymmetry: 0.0002\n",
      "  Transport Map Smoothness:\n",
      "    quality → Returns entropy: 4.6052\n",
      "    Returns → quality entropy: 4.6052\n",
      "    Asymmetry: 0.0000\n",
      "  DIVOT Decision:\n",
      "    Direction Score: 0.0104\n",
      "    Predicted: quality → Returns\n",
      "    Confidence: Moderate\n",
      "    True: quality → Returns\n",
      "    Correct: Yes\n",
      "\n",
      "Analyzing volatility <-> Returns...\n",
      "----------------------------------------\n",
      "  Transport Cost Asymmetry:\n",
      "    volatility → Returns: 0.315805\n",
      "    Returns → volatility: 0.289165\n",
      "    Asymmetry: -0.026640\n",
      "  Residual Independence Asymmetry:\n",
      "    volatility → Returns independence: 0.2204\n",
      "    Returns → volatility independence: 0.1304\n",
      "    Asymmetry: -0.0900\n",
      "  Transport Map Smoothness:\n",
      "    volatility → Returns entropy: 4.6052\n",
      "    Returns → volatility entropy: 4.6052\n",
      "    Asymmetry: 0.0000\n",
      "  DIVOT Decision:\n",
      "    Direction Score: -0.0467\n",
      "    Predicted: Returns → volatility\n",
      "    Confidence: Moderate\n",
      "    True: volatility → Returns\n",
      "    Correct: No\n",
      "\n",
      "============================================================\n",
      "DIVOT RESULTS:\n",
      "============================================================\n",
      "    Factor            Direction Confidence  Correct\n",
      "     Value      value → Returns   Moderate    False\n",
      "      Size       size → Returns   Moderate     True\n",
      "   Quality    quality → Returns   Moderate     True\n",
      "Volatility Returns → volatility   Moderate    False\n",
      "\n",
      "DIVOT Accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Run DIVOT\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: DIVOT\")\n",
    "print(\"=\" * 70)\n",
    "divot_df, divot_details = run_divot_discovery(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d41df-415e-48c9-9a51-bea0c1b198c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: COMPARISON\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "METHOD COMPARISON\n",
      "======================================================================\n",
      "    Factor       True Direction         PC Algorithm                  ANM                DIVOT PC Correct ANM Correct DIVOT Correct\n",
      "     Value       None (placebo)       Not identified      value → Returns      value → Returns          Y           N             N\n",
      "      Size       Size → Returns       Size → Returns       size → Returns       size → Returns          Y           N             N\n",
      "   Quality    Quality → Returns       Not identified         Inconclusive    quality → Returns          N           N             N\n",
      "Volatility Volatility → Returns Volatility → Returns Returns → volatility Returns → volatility          Y           N             N\n",
      "\n",
      "Method Accuracy:\n",
      "PC Algorithm: 75.0%\n",
      "ANM: 0.0%\n",
      "DIVOT: 0.0%\n",
      "Graph saved to /Users/saeedalameri/Desktop/Thesis Project/Python/Graphs/Synthetic/causal_discovery_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Compare methods\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "comparison_df, pc_acc, anm_acc, divot_acc = compare_causal_discovery_methods(\n",
    "    pc_results, anm_df, divot_df\n",
    ")\n",
    "\n",
    "# Create visualizations\n",
    "plot_method_comparison(comparison_df, pc_acc, anm_acc, divot_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48493a4-d9dd-4517-b58e-501edb8fef16",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 7. Summary\n",
    "\n",
    " Analysis demonstrates three causal discovery algorithms:\n",
    "\n",
    " - **PC Algorithm**: Identifies causal structure between multiple variables using conditional independence tests\n",
    " - **ANM**: Tests pairwise causal directions using residual independence\n",
    " - **DIVOT**: Uses optimal transport to detect causal asymmetries\n",
    "\n",
    " All methods correctly identify Value as non-causal (placebo) and detect Quality → Returns relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a5d4b-535e-41ae-9e5c-3adcec1f2234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis complete. Check 'Graphs/Synthetic' directory for visualizations.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "print(\"\\nAnalysis complete. Check 'Graphs/Synthetic' directory for visualizations.\") "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
