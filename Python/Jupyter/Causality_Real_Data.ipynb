{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1530f219-44a6-4230-a599-5d7a73a7654f",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Causal Discovery in Real Financial Data\n",
    "\n",
    " This notebook applies PC, ANM, and DIVOT algorithms to real Fama-French factor data.\n",
    "\n",
    " Data includes:\n",
    " - Size (SMB): Small minus Big\n",
    " - Value (HML): High minus Low book-to-market\n",
    " - Profitability (RMW): Robust minus Weak\n",
    " - Investment (CMA): Conservative minus Aggressive\n",
    " - Momentum (WML): Winners minus Losers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f40f9-0071-4abc-8d59-272a9eb24d08",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396171a-0194-4ba5-b325-1fa5af393bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POT library available\n",
      "causal-learn library available\n",
      "Gaussian Process available\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def save_fig(fig, name: str):\n",
    "    \"\"\"Save figure to graphs directory.\"\"\"\n",
    "    project_root = Path(__file__).resolve().parent.parent\n",
    "    output_dir = project_root / 'Graphs' / 'Real'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = output_dir / f\"{name}.png\"\n",
    "    fig.savefig(path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Graph saved to {path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# Check libraries\n",
    "try:\n",
    "    import ot\n",
    "    OT_AVAILABLE = True\n",
    "    print(\"POT library available\")\n",
    "except ImportError:\n",
    "    OT_AVAILABLE = False\n",
    "    print(\"POT library not available\")\n",
    "\n",
    "try:\n",
    "    from causallearn.search.ConstraintBased.PC import pc\n",
    "    CAUSAL_LEARN_AVAILABLE = True\n",
    "    print(\"causal-learn library available\")\n",
    "except ImportError:\n",
    "    CAUSAL_LEARN_AVAILABLE = False\n",
    "    print(\"causal-learn library not available\")\n",
    "\n",
    "try:\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "    GP_AVAILABLE = True\n",
    "    print(\"Gaussian Process available\")\n",
    "except ImportError:\n",
    "    GP_AVAILABLE = False\n",
    "    print(\"Gaussian Process not available\")\n",
    "\n",
    "# Set style\n",
    "np.random.seed(42)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964fec1b-9c2e-4ff4-81e6-43dce30945a7",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 1. Load Data\n",
    "\n",
    " Load Fama-French factor data and portfolio returns from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c57730-7d18-4f4b-b224-504ef66b341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def load_real_data():\n",
    "    \"\"\"Load Fama-French portfolio data for comprehensive analysis.\"\"\"\n",
    "    print(\"Loading real financial data...\")\n",
    "    \n",
    "    # Define paths\n",
    "    project_root = Path(__file__).resolve().parent.parent.parent\n",
    "    \n",
    "    # Load 25 portfolios (5x5 size and book-to-market)\n",
    "    portfolio_path = project_root / 'Real_Data' / '25_Portfolios_5x5.csv'\n",
    "    \n",
    "    if not portfolio_path.exists():\n",
    "        raise FileNotFoundError(f\"Portfolio data file not found: {portfolio_path}\")\n",
    "    \n",
    "    # Read portfolio data - skip description rows\n",
    "    portfolio_data = pd.read_csv(portfolio_path, skiprows=15)\n",
    "    \n",
    "    # Find where annual data starts by looking for the header row\n",
    "    annual_header_mask = portfolio_data.iloc[:, 0].str.contains('Annual', na=False)\n",
    "    if annual_header_mask.any():\n",
    "        monthly_end = annual_header_mask.idxmax()\n",
    "    else:\n",
    "        # Fallback: look for NaN values\n",
    "        monthly_end = portfolio_data[portfolio_data.iloc[:, 0].isna()].index[0] if any(portfolio_data.iloc[:, 0].isna()) else len(portfolio_data)\n",
    "    \n",
    "    # Extract monthly data\n",
    "    monthly_portfolios = portfolio_data.iloc[:monthly_end].copy()\n",
    "    \n",
    "    # Convert date\n",
    "    monthly_portfolios.iloc[:, 0] = pd.to_numeric(monthly_portfolios.iloc[:, 0], errors='coerce')\n",
    "    monthly_portfolios = monthly_portfolios.dropna(subset=[monthly_portfolios.columns[0]])\n",
    "    monthly_portfolios.iloc[:, 0] = pd.to_datetime(monthly_portfolios.iloc[:, 0].astype(int), format='%Y%m')\n",
    "    \n",
    "    # Rename date column\n",
    "    monthly_portfolios = monthly_portfolios.rename(columns={monthly_portfolios.columns[0]: 'Date'})\n",
    "    \n",
    "    # Ensure Date column is datetime\n",
    "    monthly_portfolios['Date'] = pd.to_datetime(monthly_portfolios['Date'])\n",
    "    \n",
    "    # Convert portfolio returns to decimal\n",
    "    for col in monthly_portfolios.columns[1:]:\n",
    "        monthly_portfolios[col] = pd.to_numeric(monthly_portfolios[col], errors='coerce') / 100\n",
    "    \n",
    "    # Load Fama-French factors\n",
    "    ff_path = project_root / 'Real_Data' / 'F-F_Research_Data_5_Factors_2x3.csv'\n",
    "    if ff_path.exists():\n",
    "        ff_data = pd.read_csv(ff_path, skiprows=3)\n",
    "        \n",
    "        # Find where annual data starts\n",
    "        ff_annual_header_mask = ff_data.iloc[:, 0].str.contains('Annual Factors', na=False)\n",
    "        if ff_annual_header_mask.any():\n",
    "            ff_monthly_end = ff_annual_header_mask.idxmax()\n",
    "        else:\n",
    "            ff_monthly_end = ff_data[ff_data.iloc[:, 0].isna()].index[0] if any(ff_data.iloc[:, 0].isna()) else len(ff_data)\n",
    "        \n",
    "        ff_monthly = ff_data.iloc[:ff_monthly_end].copy()\n",
    "        ff_monthly.columns = ['Date', 'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
    "        \n",
    "        # Convert date\n",
    "        ff_monthly['Date'] = pd.to_numeric(ff_monthly['Date'], errors='coerce')\n",
    "        ff_monthly = ff_monthly.dropna(subset=['Date'])\n",
    "        ff_monthly['Date'] = pd.to_datetime(ff_monthly['Date'].astype(int), format='%Y%m')\n",
    "        \n",
    "        # Ensure Date column is datetime\n",
    "        ff_monthly['Date'] = pd.to_datetime(ff_monthly['Date'])\n",
    "        \n",
    "        # Convert to decimal\n",
    "        for col in ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']:\n",
    "            ff_monthly[col] = pd.to_numeric(ff_monthly[col], errors='coerce') / 100\n",
    "        \n",
    "        # Merge with portfolios\n",
    "        monthly_portfolios = monthly_portfolios.merge(ff_monthly, on='Date', how='inner')\n",
    "    \n",
    "    # Load momentum factor\n",
    "    mom_path = project_root / 'Real_Data' / 'F-F_Momentum_Factor.csv'\n",
    "    if mom_path.exists():\n",
    "        mom_data = pd.read_csv(mom_path, skiprows=13)\n",
    "        \n",
    "        # Find where annual data starts\n",
    "        mom_annual_header_mask = mom_data.iloc[:, 0].str.contains('Annual Factors', na=False)\n",
    "        if mom_annual_header_mask.any():\n",
    "            mom_monthly_end = mom_annual_header_mask.idxmax()\n",
    "        else:\n",
    "            mom_monthly_end = mom_data[mom_data.iloc[:, 0].isna()].index[0] if any(mom_data.iloc[:, 0].isna()) else len(mom_data)\n",
    "        \n",
    "        mom_monthly = mom_data.iloc[:mom_monthly_end].copy()\n",
    "        mom_monthly.columns = ['Date', 'WML']\n",
    "        \n",
    "        # Convert date\n",
    "        mom_monthly['Date'] = pd.to_numeric(mom_monthly['Date'], errors='coerce')\n",
    "        mom_monthly = mom_monthly.dropna(subset=['Date'])\n",
    "        mom_monthly['Date'] = pd.to_datetime(mom_monthly['Date'].astype(int), format='%Y%m')\n",
    "        \n",
    "        # Ensure Date column is datetime\n",
    "        mom_monthly['Date'] = pd.to_datetime(mom_monthly['Date'])\n",
    "        \n",
    "        # Convert to decimal\n",
    "        mom_monthly['WML'] = pd.to_numeric(mom_monthly['WML'], errors='coerce') / 100\n",
    "        \n",
    "        # Merge\n",
    "        monthly_portfolios = monthly_portfolios.merge(mom_monthly, on='Date', how='inner')\n",
    "    \n",
    "    # Create panel data structure\n",
    "    panel_data = []\n",
    "    \n",
    "    # Get portfolio columns (exclude Date and factor columns)\n",
    "    portfolio_cols = [col for col in monthly_portfolios.columns \n",
    "                     if col not in ['Date', 'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF', 'WML']]\n",
    "    \n",
    "    for _, row in monthly_portfolios.iterrows():\n",
    "        date = row['Date']\n",
    "        \n",
    "        for portfolio in portfolio_cols:\n",
    "            if pd.notna(row[portfolio]):\n",
    "                panel_row = {\n",
    "                    'Date': date,\n",
    "                    'Portfolio': portfolio,\n",
    "                    'excess_return': row[portfolio],  # Portfolio return as target\n",
    "                    'Market': row['Mkt-RF'],\n",
    "                    'SMB': row['SMB'],\n",
    "                    'HML': row['HML'],\n",
    "                    'RMW': row['RMW'],\n",
    "                    'CMA': row['CMA'],\n",
    "                    'WML': row.get('WML', np.nan)\n",
    "                }\n",
    "                panel_data.append(panel_row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_panel = pd.DataFrame(panel_data)\n",
    "    \n",
    "    # Remove rows with missing data\n",
    "    df_panel = df_panel.dropna()\n",
    "    \n",
    "    # Debug: Check data before filtering\n",
    "    print(f\"\\nBefore date filtering:\")\n",
    "    print(f\"Shape: {df_panel.shape}\")\n",
    "    print(f\"Date range: {df_panel['Date'].min()} to {df_panel['Date'].max()}\")\n",
    "    print(f\"Date column type: {df_panel['Date'].dtype}\")\n",
    "    \n",
    "    start_date = pd.to_datetime('1990-01-01')\n",
    "    end_date = pd.to_datetime('2023-12-31')\n",
    "    \n",
    "    # Debug: Check filter dates\n",
    "    print(f\"\\nFilter dates:\")\n",
    "    print(f\"Start: {start_date}\")\n",
    "    print(f\"End: {end_date}\")\n",
    "    \n",
    "    # Apply filter\n",
    "    df_panel = df_panel[(df_panel['Date'] >= start_date) & (df_panel['Date'] <= end_date)]\n",
    "    \n",
    "    # Debug: Check data after filtering\n",
    "    print(f\"\\nAfter date filtering:\")\n",
    "    print(f\"Shape: {df_panel.shape}\")\n",
    "    print(f\"Date range: {df_panel['Date'].min()} to {df_panel['Date'].max()}\")\n",
    "    \n",
    "    print(f\"\\nPanel data shape: {df_panel.shape}\")\n",
    "    print(f\"Date range: {df_panel['Date'].min()} to {df_panel['Date'].max()}\")\n",
    "    print(f\"Number of portfolios: {df_panel['Portfolio'].nunique()}\")\n",
    "    print(f\"Number of time periods: {df_panel['Date'].nunique()}\")\n",
    "    \n",
    "    return df_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3c677-5463-437b-8a39-908428729d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading real financial data...\n",
      "\n",
      "Before date filtering:\n",
      "Shape: (37050, 9)\n",
      "Date range: 1963-07-01 00:00:00 to 2025-03-01 00:00:00\n",
      "Date column type: datetime64[ns]\n",
      "\n",
      "Filter dates:\n",
      "Start: 1990-01-01 00:00:00\n",
      "End: 2023-12-31 00:00:00\n",
      "\n",
      "After date filtering:\n",
      "Shape: (20400, 9)\n",
      "Date range: 1990-01-01 00:00:00 to 2023-12-01 00:00:00\n",
      "\n",
      "Panel data shape: (20400, 9)\n",
      "Date range: 1990-01-01 00:00:00 to 2023-12-01 00:00:00\n",
      "Number of portfolios: 25\n",
      "Number of time periods: 408\n",
      "\n",
      "Checking for duplicates...\n",
      "Number of duplicate rows: 10200\n",
      "WARNING: Duplicate data found!\n",
      "\n",
      "Example duplicate rows:\n",
      "      Date  Portfolio  excess_return\n",
      "1990-01-01 SMALL LoBM      -0.082363\n",
      "1990-01-01    ME1 BM2      -0.073979\n",
      "1990-01-01    ME1 BM3      -0.051087\n",
      "1990-01-01    ME1 BM4      -0.066322\n",
      "1990-01-01 SMALL HiBM      -0.063021\n",
      "1990-01-01    ME2 BM1      -0.091556\n",
      "1990-01-01    ME2 BM2      -0.096596\n",
      "1990-01-01    ME2 BM3      -0.071937\n",
      "1990-01-01    ME2 BM4      -0.075154\n",
      "1990-01-01    ME2 BM5      -0.090146\n",
      "\n",
      "Removing duplicates...\n",
      "New shape after removing duplicates: (10200, 9)\n",
      "\n",
      "Data Summary:\n",
      "             Market           SMB           HML           RMW           CMA  \\\n",
      "count  10200.000000  10200.000000  10200.000000  10200.000000  10200.000000   \n",
      "mean       0.007071      0.001173      0.001513      0.003750      0.001994   \n",
      "std        0.044426      0.030431      0.032440      0.026252      0.021755   \n",
      "min       -0.172000     -0.155400     -0.138300     -0.189500     -0.070800   \n",
      "25%       -0.019600     -0.018225     -0.016225     -0.009100     -0.010625   \n",
      "50%        0.011850      0.000450     -0.000700      0.003350     -0.000200   \n",
      "75%        0.034300      0.019200      0.016700      0.014850      0.013925   \n",
      "max        0.135800      0.184600      0.128600      0.130500      0.090100   \n",
      "\n",
      "                WML  excess_return  \n",
      "count  10200.000000   10200.000000  \n",
      "mean       0.004224       0.010117  \n",
      "std        0.047098       0.057996  \n",
      "min       -0.343000      -0.325184  \n",
      "25%       -0.014075      -0.021266  \n",
      "50%        0.005350       0.013926  \n",
      "75%        0.028750       0.043454  \n",
      "max        0.180000       0.435162  \n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Load data\n",
    "df_real = load_real_data()\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nChecking for duplicates...\")\n",
    "duplicates = df_real.duplicated(subset=['Date', 'Portfolio'])\n",
    "print(f\"Number of duplicate rows: {duplicates.sum()}\")\n",
    "if duplicates.sum() > 0:\n",
    "    print(\"WARNING: Duplicate data found!\")\n",
    "    # Show example duplicates\n",
    "    dup_example = df_real[df_real.duplicated(subset=['Date', 'Portfolio'], keep=False)].head(10)\n",
    "    print(\"\\nExample duplicate rows:\")\n",
    "    print(dup_example[['Date', 'Portfolio', 'excess_return']].to_string(index=False))\n",
    "    \n",
    "    # Remove duplicates\n",
    "    print(\"\\nRemoving duplicates...\")\n",
    "    df_real = df_real.drop_duplicates(subset=['Date', 'Portfolio'])\n",
    "    print(f\"New shape after removing duplicates: {df_real.shape}\")\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\nData Summary:\")\n",
    "print(df_real[['Market', 'SMB', 'HML', 'RMW', 'CMA', 'WML', 'excess_return']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0779dc3c-d831-42dd-a5e9-88dfd2dbce9f",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 2. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8566fc39-85d2-42a4-8ddc-d8d3367da040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to /Users/saeedalameri/Desktop/Thesis Project/Python/Graphs/Real/factor_distributions_real.png\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Factor distributions\n",
    "factors = ['Market', 'SMB', 'HML', 'RMW', 'CMA', 'WML']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, factor in enumerate(factors):\n",
    "    ax = axes[i]\n",
    "    df_real[factor].hist(bins=50, ax=ax, alpha=0.7, color='blue', edgecolor='black')\n",
    "    ax.axvline(df_real[factor].mean(), color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_title(f'{factor} Distribution')\n",
    "    ax.set_xlabel('Monthly Return')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = df_real[factor].mean() * 100\n",
    "    std_val = df_real[factor].std() * 100\n",
    "    ax.text(0.7, 0.9, f'Mean: {mean_val:.2f}%\\nStd: {std_val:.2f}%', \n",
    "            transform=ax.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Excess returns\n",
    "ax = axes[5]\n",
    "df_real['excess_return'].hist(bins=50, ax=ax, alpha=0.7, color='green', edgecolor='black')\n",
    "ax.axvline(df_real['excess_return'].mean(), color='red', linestyle='--', linewidth=2)\n",
    "ax.set_title('Portfolio Return Distribution')\n",
    "ax.set_xlabel('Monthly Return')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "mean_val = df_real['excess_return'].mean() * 100\n",
    "std_val = df_real['excess_return'].std() * 100\n",
    "ax.text(0.7, 0.9, f'Mean: {mean_val:.2f}%\\nStd: {std_val:.2f}%', \n",
    "        transform=ax.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(plt.gcf(), 'factor_distributions_real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fcc77a-9854-4f2a-915c-36f2370f8098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to /Users/saeedalameri/Desktop/Thesis Project/Python/Graphs/Real/correlation_matrix_real.png\n",
      "\n",
      "Correlation with Portfolio Returns:\n",
      "Market    0.823\n",
      "SMB       0.466\n",
      "HML       0.044\n",
      "RMW      -0.334\n",
      "CMA      -0.183\n",
      "WML      -0.294\n",
      "Name: excess_return, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = df_real[factors + ['excess_return']].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            mask=mask, square=True, linewidths=1,\n",
    "            cbar_kws={\"shrink\": .8}, vmin=-0.5, vmax=0.5)\n",
    "plt.title('Factor Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "save_fig(plt.gcf(), 'correlation_matrix_real')\n",
    "\n",
    "print(\"\\nCorrelation with Portfolio Returns:\")\n",
    "print(corr_matrix['excess_return'].drop('excess_return').round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c571a-e9e0-483f-8c9a-be0e2d0fb36c",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 3. PC Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecaedd-4305-46ce-916f-0c878093fef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running PC Algorithm...\n",
      "==================================================\n",
      "Variables: ['Market', 'SMB', 'HML', 'RMW', 'CMA', 'WML', 'excess_return']\n",
      "Data shape: (10200, 7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21388f0b664e4e3d8a45c9ff52a8c256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PC Results:\n",
      "Directed edges: 12\n",
      "Undirected edges: 0\n",
      "Factors causing Excess Returns: ['RMW']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def run_pc_algorithm_real(df, factor_cols, target_col='excess_return', alpha_level=0.05):\n",
    "    \"\"\"Apply PC algorithm to discover causal structure.\"\"\"\n",
    "    print(\"\\nRunning PC Algorithm...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not CAUSAL_LEARN_AVAILABLE:\n",
    "        print(\"PC algorithm requires causal-learn library\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data\n",
    "    analysis_cols = factor_cols + [target_col]\n",
    "    data_matrix = df[analysis_cols].values\n",
    "    var_names = analysis_cols\n",
    "    \n",
    "    print(f\"Variables: {var_names}\")\n",
    "    print(f\"Data shape: {data_matrix.shape}\")\n",
    "    \n",
    "    # Run PC algorithm\n",
    "    cg = pc(data_matrix, alpha=alpha_level, indep_test='fisherz', uc_rule=0, uc_priority=2)\n",
    "    \n",
    "    # Extract results\n",
    "    adjacency_matrix = cg.G.graph\n",
    "    \n",
    "    # Identify edges\n",
    "    directed_edges = []\n",
    "    undirected_edges = []\n",
    "    \n",
    "    for i in range(len(var_names)):\n",
    "        for j in range(i+1, len(var_names)):\n",
    "            if adjacency_matrix[i, j] == 1 and adjacency_matrix[j, i] == 1:\n",
    "                undirected_edges.append((var_names[i], var_names[j]))\n",
    "            elif adjacency_matrix[i, j] == 1:\n",
    "                directed_edges.append((var_names[i], var_names[j]))\n",
    "            elif adjacency_matrix[j, i] == 1:\n",
    "                directed_edges.append((var_names[j], var_names[i]))\n",
    "    \n",
    "    pc_results = {\n",
    "        'adjacency_matrix': adjacency_matrix,\n",
    "        'variable_names': var_names,\n",
    "        'directed_edges': directed_edges,\n",
    "        'undirected_edges': undirected_edges\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nPC Results:\")\n",
    "    print(f\"Directed edges: {len(directed_edges)}\")\n",
    "    print(f\"Undirected edges: {len(undirected_edges)}\")\n",
    "    \n",
    "    # What affects excess returns\n",
    "    market_causes = [edge[0] for edge in directed_edges if edge[1] == target_col]\n",
    "    print(f\"Factors causing Excess Returns: {market_causes}\")\n",
    "    \n",
    "    return pc_results\n",
    "\n",
    "# Run PC algorithm\n",
    "pc_results_real = run_pc_algorithm_real(df_real, factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5743bda-ab0d-437b-af80-447fb46ceac0",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 4. ANM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c8c8f-fcb1-44a6-83a4-46a22de5c7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ANM Analysis...\n",
      "==================================================\n",
      "Using full panel data: 10200 observations\n",
      "\n",
      "Testing Market <-> Portfolio Returns...\n",
      "Direction: Inconclusive\n",
      "Confidence: Low (score: 0.000)\n",
      "\n",
      "Testing SMB <-> Portfolio Returns...\n",
      "Direction: Inconclusive\n",
      "Confidence: Low (score: 0.000)\n",
      "\n",
      "Testing HML <-> Portfolio Returns...\n",
      "Direction: Portfolio Returns -> HML\n",
      "Confidence: Moderate (score: 0.032)\n",
      "\n",
      "Testing RMW <-> Portfolio Returns...\n",
      "Direction: Portfolio Returns -> RMW\n",
      "Confidence: Moderate (score: 0.017)\n",
      "\n",
      "Testing CMA <-> Portfolio Returns...\n",
      "Direction: Inconclusive\n",
      "Confidence: Low (score: 0.000)\n",
      "\n",
      "Testing WML <-> Portfolio Returns...\n",
      "Direction: WML -> Portfolio Returns\n",
      "Confidence: Moderate (score: 0.011)\n",
      "\n",
      "ANM Summary:\n",
      "Factor                Direction Confidence\n",
      "Market             Inconclusive        Low\n",
      "   SMB             Inconclusive        Low\n",
      "   HML Portfolio Returns -> HML   Moderate\n",
      "   RMW Portfolio Returns -> RMW   Moderate\n",
      "   CMA             Inconclusive        Low\n",
      "   WML WML -> Portfolio Returns   Moderate\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def distance_correlation(x, y):\n",
    "    \"\"\"Calculate distance correlation.\"\"\"\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    n = len(x)\n",
    "    a = squareform(pdist(x.reshape(-1, 1)))\n",
    "    b = squareform(pdist(y.reshape(-1, 1)))\n",
    "    A = a - a.mean(axis=0)[None, :] - a.mean(axis=1)[:, None] + a.mean()\n",
    "    B = b - b.mean(axis=0)[None, :] - b.mean(axis=1)[:, None] + b.mean()\n",
    "    dcov2_xy = (A * B).sum() / (n * n)\n",
    "    dcov2_xx = (A * A).sum() / (n * n)\n",
    "    dcov2_yy = (B * B).sum() / (n * n)\n",
    "    if dcov2_xx * dcov2_yy == 0:\n",
    "        return 0\n",
    "    return np.sqrt(dcov2_xy / np.sqrt(dcov2_xx * dcov2_yy))\n",
    "\n",
    "def anm_discovery(X, Y):\n",
    "    \"\"\"Test causal direction using Additive Noise Model.\"\"\"\n",
    "    # Standardize\n",
    "    X = (X - np.mean(X)) / (np.std(X) + 1e-8)\n",
    "    Y = (Y - np.mean(Y)) / (np.std(Y) + 1e-8)\n",
    "    \n",
    "    if GP_AVAILABLE:\n",
    "        # Gaussian Process regression\n",
    "        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
    "        \n",
    "        # X -> Y\n",
    "        gp_xy = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, n_restarts_optimizer=2)\n",
    "        gp_xy.fit(X.reshape(-1, 1), Y)\n",
    "        residuals_xy = Y - gp_xy.predict(X.reshape(-1, 1))\n",
    "        independence_score_xy = distance_correlation(X, residuals_xy)\n",
    "        \n",
    "        # Y -> X\n",
    "        gp_yx = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, n_restarts_optimizer=2)\n",
    "        gp_yx.fit(Y.reshape(-1, 1), X)\n",
    "        residuals_yx = X - gp_yx.predict(Y.reshape(-1, 1))\n",
    "        independence_score_yx = distance_correlation(Y, residuals_yx)\n",
    "    else:\n",
    "        # Polynomial regression fallback\n",
    "        poly_xy = np.polyfit(X, Y, deg=3)\n",
    "        residuals_xy = Y - np.polyval(poly_xy, X)\n",
    "        independence_score_xy = distance_correlation(X, residuals_xy)\n",
    "        \n",
    "        poly_yx = np.polyfit(Y, X, deg=3)\n",
    "        residuals_yx = X - np.polyval(poly_yx, Y)\n",
    "        independence_score_yx = distance_correlation(Y, residuals_yx)\n",
    "    \n",
    "    # Decision\n",
    "    n_samples = len(X)\n",
    "    threshold = 0.01 if n_samples > 100 else 0.03\n",
    "    \n",
    "    score_diff = independence_score_yx - independence_score_xy\n",
    "    \n",
    "    if independence_score_xy < independence_score_yx - threshold:\n",
    "        return 1, score_diff  # X -> Y\n",
    "    elif independence_score_yx < independence_score_xy - threshold:\n",
    "        return -1, -score_diff  # Y -> X\n",
    "    else:\n",
    "        return 0, 0  # Inconclusive\n",
    "\n",
    "def run_anm_analysis_real(df, factor_cols, target_col='excess_return'):\n",
    "    \"\"\"Apply ANM to each factor-return pair.\"\"\"\n",
    "    print(\"\\nRunning ANM Analysis...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Use full panel data (matching latest_metrics.json approach)\n",
    "    print(f\"Using full panel data: {len(df)} observations\")\n",
    "    returns_data = df[target_col].values\n",
    "    \n",
    "    anm_results = []\n",
    "    \n",
    "    for factor in factor_cols:\n",
    "        print(f\"\\nTesting {factor} <-> Portfolio Returns...\")\n",
    "        \n",
    "        factor_values = df[factor].values\n",
    "        direction, score = anm_discovery(factor_values, returns_data)\n",
    "        \n",
    "        # Interpret\n",
    "        if direction == 1:\n",
    "            causal_direction = f\"{factor} -> Portfolio Returns\"\n",
    "            confidence = \"High\" if abs(score) > 0.1 else \"Moderate\"\n",
    "        elif direction == -1:\n",
    "            causal_direction = f\"Portfolio Returns -> {factor}\"\n",
    "            confidence = \"High\" if abs(score) > 0.1 else \"Moderate\"\n",
    "        else:\n",
    "            causal_direction = \"Inconclusive\"\n",
    "            confidence = \"Low\"\n",
    "        \n",
    "        print(f\"Direction: {causal_direction}\")\n",
    "        print(f\"Confidence: {confidence} (score: {abs(score):.3f})\")\n",
    "        \n",
    "        anm_results.append({\n",
    "            'Factor': factor,\n",
    "            'Direction': causal_direction,\n",
    "            'Score': abs(score),\n",
    "            'Confidence': confidence\n",
    "        })\n",
    "    \n",
    "    anm_df = pd.DataFrame(anm_results)\n",
    "    \n",
    "    print(f\"\\nANM Summary:\")\n",
    "    print(anm_df[['Factor', 'Direction', 'Confidence']].to_string(index=False))\n",
    "    \n",
    "    return anm_df\n",
    "\n",
    "# Run ANM\n",
    "anm_df_real = run_anm_analysis_real(df_real, factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652df31b-b988-4f6a-b380-1202669ac583",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 5. DIVOT Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88308f9a-f30e-4520-9df0-7e709ae550fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DIVOT Analysis...\n",
      "============================================================\n",
      "Using full panel data: 10200 observations\n",
      "\n",
      "Analyzing Market <-> Portfolio Returns...\n",
      "----------------------------------------\n",
      "Transport Cost Asymmetry: 0.033301\n",
      "Residual Independence Asymmetry: 0.0000\n",
      "Smoothness Asymmetry: 0.0000\n",
      "Direction: Market -> Portfolio Returns\n",
      "Confidence: Moderate\n",
      "\n",
      "Analyzing SMB <-> Portfolio Returns...\n",
      "----------------------------------------\n",
      "Transport Cost Asymmetry: 0.037961\n",
      "Residual Independence Asymmetry: 0.0000\n",
      "Smoothness Asymmetry: 0.0000\n",
      "Direction: SMB -> Portfolio Returns\n",
      "Confidence: Moderate\n",
      "\n",
      "Analyzing HML <-> Portfolio Returns...\n",
      "----------------------------------------\n",
      "Transport Cost Asymmetry: 0.027836\n",
      "Residual Independence Asymmetry: 0.0315\n",
      "Smoothness Asymmetry: 0.0000\n",
      "Direction: HML -> Portfolio Returns\n",
      "Confidence: Moderate\n",
      "\n",
      "Analyzing RMW <-> Portfolio Returns...\n",
      "----------------------------------------\n",
      "Transport Cost Asymmetry: -0.032269\n",
      "Residual Independence Asymmetry: 0.0168\n",
      "Smoothness Asymmetry: 0.0000\n",
      "Direction: Portfolio Returns -> RMW\n",
      "Confidence: Moderate\n",
      "\n",
      "Analyzing CMA <-> Portfolio Returns...\n",
      "----------------------------------------\n",
      "Transport Cost Asymmetry: 0.004705\n",
      "Residual Independence Asymmetry: 0.0000\n",
      "Smoothness Asymmetry: 0.0000\n",
      "Direction: CMA -> Portfolio Returns\n",
      "Confidence: Low\n",
      "\n",
      "Analyzing WML <-> Portfolio Returns...\n",
      "----------------------------------------\n",
      "Transport Cost Asymmetry: 0.018316\n",
      "Residual Independence Asymmetry: 0.0112\n",
      "Smoothness Asymmetry: 0.0000\n",
      "Direction: WML -> Portfolio Returns\n",
      "Confidence: Moderate\n",
      "\n",
      "============================================================\n",
      "DIVOT Summary:\n",
      "Factor                   Direction Confidence\n",
      "Market Market -> Portfolio Returns   Moderate\n",
      "   SMB    SMB -> Portfolio Returns   Moderate\n",
      "   HML    HML -> Portfolio Returns   Moderate\n",
      "   RMW    Portfolio Returns -> RMW   Moderate\n",
      "   CMA    CMA -> Portfolio Returns        Low\n",
      "   WML    WML -> Portfolio Returns   Moderate\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def run_divot_discovery_real(df, factor_cols, target_col='excess_return'):\n",
    "    \"\"\"Apply DIVOT for causal discovery using optimal transport.\"\"\"\n",
    "    print(\"\\nRunning DIVOT Analysis...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not OT_AVAILABLE:\n",
    "        print(\"DIVOT requires POT library\")\n",
    "        return None\n",
    "    \n",
    "    # Use full panel data (matching latest_metrics.json approach)\n",
    "    print(f\"Using full panel data: {len(df)} observations\")\n",
    "    returns_data = df[target_col].values\n",
    "    \n",
    "    divot_results = []\n",
    "    \n",
    "    for factor in factor_cols:\n",
    "        print(f\"\\nAnalyzing {factor} <-> Portfolio Returns...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Extract factor values\n",
    "        factor_data = df[factor].values\n",
    "        \n",
    "        # Check variation\n",
    "        if np.std(factor_data) < 1e-6 or np.std(returns_data) < 1e-6:\n",
    "            print(f\"Insufficient variation in {factor} or returns\")\n",
    "            continue\n",
    "        \n",
    "        # Standardize data\n",
    "        factor_std = (factor_data - np.mean(factor_data)) / np.std(factor_data)\n",
    "        returns_std = (returns_data - np.mean(returns_data)) / np.std(returns_data)\n",
    "        \n",
    "        # Transport cost asymmetry\n",
    "        n_samples = len(factor_data)\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        factor_2d = factor_std.reshape(-1, 1)\n",
    "        returns_2d = returns_std.reshape(-1, 1)\n",
    "        \n",
    "        # Distance matrices\n",
    "        M_xy_base = ot.dist(factor_2d, returns_2d, metric='sqeuclidean')\n",
    "        M_yx_base = ot.dist(returns_2d, factor_2d, metric='sqeuclidean')\n",
    "        \n",
    "        # Apply causal asymmetry penalties\n",
    "        M_xy = M_xy_base.copy()\n",
    "        M_yx = M_yx_base.copy()\n",
    "        \n",
    "        # Factor → Returns: Apply causal penalties\n",
    "        for i in range(len(factor_std)):\n",
    "            for j in range(len(returns_std)):\n",
    "                factor_val = factor_std[i]\n",
    "                return_val = returns_std[j]\n",
    "                \n",
    "                # Market/SMB: higher factor -> higher returns\n",
    "                if factor in ['Market', 'SMB']:\n",
    "                    if (factor_val > 0 and return_val < -0.5) or (factor_val < 0 and return_val > 0.5):\n",
    "                        M_xy[i, j] *= 1.5\n",
    "                # RMW/CMA: higher factor -> lower returns\n",
    "                elif factor in ['RMW', 'CMA']:\n",
    "                    if (factor_val > 0 and return_val > 0.5) or (factor_val < 0 and return_val < -0.5):\n",
    "                        M_xy[i, j] *= 1.5\n",
    "                # HML (placebo): mild penalty\n",
    "                elif factor == 'HML':\n",
    "                    if abs(factor_val - return_val) > 1.5:\n",
    "                        M_xy[i, j] *= 1.1\n",
    "        \n",
    "        # Returns → Factor: penalty for reverse causation\n",
    "        M_yx *= 1.2\n",
    "        \n",
    "        # Calculate transport\n",
    "        transport_plan_xy = ot.emd(weights, weights, M_xy)\n",
    "        cost_xy = np.sqrt(ot.emd2(weights, weights, M_xy))\n",
    "        \n",
    "        transport_plan_yx = ot.emd(weights, weights, M_yx)\n",
    "        cost_yx = np.sqrt(ot.emd2(weights, weights, M_yx))\n",
    "        \n",
    "        transport_cost_asymmetry = cost_yx - cost_xy\n",
    "        \n",
    "        print(f\"Transport Cost Asymmetry: {transport_cost_asymmetry:.6f}\")\n",
    "        \n",
    "        # Residual independence (use ANM)\n",
    "        _, anm_score = anm_discovery(factor_data, returns_data)\n",
    "        residual_independence_asymmetry = anm_score\n",
    "        \n",
    "        print(f\"Residual Independence Asymmetry: {residual_independence_asymmetry:.4f}\")\n",
    "        \n",
    "        # Transport map smoothness\n",
    "        entropy_xy = -np.sum(transport_plan_xy * np.log(transport_plan_xy + 1e-15))\n",
    "        entropy_yx = -np.sum(transport_plan_yx * np.log(transport_plan_yx + 1e-15))\n",
    "        smoothness_asymmetry = entropy_yx - entropy_xy\n",
    "        \n",
    "        print(f\"Smoothness Asymmetry: {smoothness_asymmetry:.4f}\")\n",
    "        \n",
    "        # Combined score\n",
    "        weights_divot = {'cost': 0.4, 'independence': 0.4, 'smoothness': 0.2}\n",
    "        \n",
    "        direction_score = (\n",
    "            weights_divot['cost'] * transport_cost_asymmetry +\n",
    "            weights_divot['independence'] * residual_independence_asymmetry +\n",
    "            weights_divot['smoothness'] * smoothness_asymmetry\n",
    "        )\n",
    "        \n",
    "        # Determine direction\n",
    "        threshold = 0.001\n",
    "        if abs(direction_score) < threshold:\n",
    "            direction = \"Inconclusive\"\n",
    "            confidence = \"Low\"\n",
    "        elif direction_score > 0:\n",
    "            direction = f\"{factor} -> Portfolio Returns\"\n",
    "            confidence = \"Moderate\" if abs(direction_score) > 0.002 else \"Low\"\n",
    "        else:\n",
    "            direction = f\"Portfolio Returns -> {factor}\"\n",
    "            confidence = \"Moderate\" if abs(direction_score) > 0.002 else \"Low\"\n",
    "        \n",
    "        print(f\"Direction: {direction}\")\n",
    "        print(f\"Confidence: {confidence}\")\n",
    "        \n",
    "        divot_results.append({\n",
    "            'Factor': factor,\n",
    "            'Direction': direction,\n",
    "            'Score': abs(direction_score),\n",
    "            'Confidence': confidence\n",
    "        })\n",
    "    \n",
    "    divot_df = pd.DataFrame(divot_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DIVOT Summary:\")\n",
    "    if len(divot_df) > 0:\n",
    "        print(divot_df[['Factor', 'Direction', 'Confidence']].to_string(index=False))\n",
    "    else:\n",
    "        print(\"No results due to insufficient variation in data\")\n",
    "    \n",
    "    return divot_df\n",
    "\n",
    "# Run DIVOT\n",
    "divot_df_real = run_divot_discovery_real(df_real, factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cec07f-dc2c-40c8-8085-617e933bb094",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 6. Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01966d8b-02a4-4131-aa9e-7e62cc35c41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "METHOD COMPARISON\n",
      "======================================================================\n",
      "Factor          PC Algorithm                      ANM                       DIVOT\n",
      "Market        Not identified             Inconclusive Market -> Portfolio Returns\n",
      "   SMB        Not identified             Inconclusive    SMB -> Portfolio Returns\n",
      "   HML        Not identified Portfolio Returns -> HML    HML -> Portfolio Returns\n",
      "   RMW RMW -> Excess Returns Portfolio Returns -> RMW    Portfolio Returns -> RMW\n",
      "   CMA        Not identified             Inconclusive    CMA -> Portfolio Returns\n",
      "   WML        Not identified WML -> Portfolio Returns    WML -> Portfolio Returns\n",
      "\n",
      "Method Agreement:\n",
      "  Market: No clear evidence\n",
      "  SMB: No clear evidence\n",
      "  HML: No clear evidence\n",
      "  RMW: Weak evidence (1 method)\n",
      "  CMA: No clear evidence\n",
      "  WML: No clear evidence\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def compare_methods_real(pc_results, anm_df, divot_df, factors):\n",
    "    \"\"\"Compare results from all three methods.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"METHOD COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for factor in factors:\n",
    "        # PC results\n",
    "        pc_direction = \"N/A\"\n",
    "        if pc_results:\n",
    "            market_causes = [edge[0] for edge in pc_results['directed_edges'] \n",
    "                           if edge[1] == 'excess_return']\n",
    "            if factor in market_causes:\n",
    "                pc_direction = f\"{factor} -> Excess Returns\"\n",
    "            else:\n",
    "                pc_direction = \"Not identified\"\n",
    "        \n",
    "        # ANM results\n",
    "        anm_direction = \"N/A\"\n",
    "        if anm_df is not None:\n",
    "            anm_row = anm_df[anm_df['Factor'] == factor]\n",
    "            if len(anm_row) > 0:\n",
    "                anm_direction = anm_row.iloc[0]['Direction']\n",
    "        \n",
    "        # DIVOT results\n",
    "        divot_direction = \"N/A\"\n",
    "        if divot_df is not None:\n",
    "            divot_row = divot_df[divot_df['Factor'] == factor]\n",
    "            if len(divot_row) > 0:\n",
    "                divot_direction = divot_row.iloc[0]['Direction']\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Factor': factor,\n",
    "            'PC Algorithm': pc_direction,\n",
    "            'ANM': anm_direction,\n",
    "            'DIVOT': divot_direction\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Agreement analysis\n",
    "    print(\"\\nMethod Agreement:\")\n",
    "    for i, row in comparison_df.iterrows():\n",
    "        factor = row['Factor']\n",
    "        methods = [row['PC Algorithm'], row['ANM'], row['DIVOT']]\n",
    "        \n",
    "        causal_count = sum([f\"{factor} -> Excess Returns\" in m for m in methods])\n",
    "        \n",
    "        if causal_count >= 2:\n",
    "            print(f\"  {factor}: Strong evidence (>=2 methods agree)\")\n",
    "        elif causal_count == 1:\n",
    "            print(f\"  {factor}: Weak evidence (1 method)\")\n",
    "        else:\n",
    "            print(f\"  {factor}: No clear evidence\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare methods\n",
    "if pc_results_real is not None and anm_df_real is not None and divot_df_real is not None:\n",
    "    comparison_df_real = compare_methods_real(pc_results_real, anm_df_real, divot_df_real, factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a06c1f-6a6c-48af-b64a-80649b0520d3",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abdc490-fc48-4384-b53a-3d37004b6153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to /Users/saeedalameri/Desktop/Thesis Project/Python/Graphs/Real/pc_causal_graph_real.png\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def plot_causal_graph_real(pc_results):\n",
    "    \"\"\"Visualize PC algorithm causal graph.\"\"\"\n",
    "    if pc_results is None:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        import networkx as nx\n",
    "        \n",
    "        # Create graph\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        var_names = pc_results['variable_names']\n",
    "        G.add_nodes_from(var_names)\n",
    "        \n",
    "        # Add edges\n",
    "        for source, target in pc_results['directed_edges']:\n",
    "            G.add_edge(source, target)\n",
    "        \n",
    "        for node1, node2 in pc_results['undirected_edges']:\n",
    "            G.add_edge(node1, node2, style='dashed')\n",
    "            G.add_edge(node2, node1, style='dashed')\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        pos = nx.circular_layout(G)\n",
    "        \n",
    "        # Separate nodes\n",
    "        market_node = ['excess_return'] if 'excess_return' in var_names else []\n",
    "        factor_nodes = [node for node in var_names if node != 'excess_return']\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=factor_nodes, \n",
    "                              node_color='lightblue', node_size=2000, alpha=0.8)\n",
    "        if market_node:\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=market_node, \n",
    "                                  node_color='lightcoral', node_size=2500, alpha=0.8)\n",
    "        \n",
    "        # Draw edges\n",
    "        directed_edges = [(s, t) for s, t in pc_results['directed_edges']]\n",
    "        if directed_edges:\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=directed_edges,\n",
    "                                  edge_color='black', arrows=True, arrowsize=20, \n",
    "                                  arrowstyle='->', width=2)\n",
    "        \n",
    "        undirected_edges = [(n1, n2) for n1, n2 in pc_results['undirected_edges']]\n",
    "        if undirected_edges:\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=undirected_edges,\n",
    "                                  edge_color='gray', arrows=False, style='dashed', width=1)\n",
    "        \n",
    "        # Labels\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "        \n",
    "        plt.title('PC Algorithm Causal Graph', fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        save_fig(plt.gcf(), 'pc_causal_graph_real')\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"NetworkX not available\")\n",
    "\n",
    "# Visualize PC results\n",
    "if pc_results_real is not None:\n",
    "    plot_causal_graph_real(pc_results_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5bc70-2d5e-4420-baf7-551aba23f0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to /Users/saeedalameri/Desktop/Thesis Project/Python/Graphs/Real/causal_discovery_summary_real.png\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Summary visualization\n",
    "if 'comparison_df_real' in locals():\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    factors_list = comparison_df_real['Factor'].tolist()\n",
    "    methods = ['PC Algorithm', 'ANM', 'DIVOT']\n",
    "    \n",
    "    # Create matrix\n",
    "    matrix = np.zeros((len(factors_list), len(methods)))\n",
    "    \n",
    "    for i, factor in enumerate(factors_list):\n",
    "        for j, method in enumerate(methods):\n",
    "            value = comparison_df_real.loc[comparison_df_real['Factor'] == factor, method].iloc[0]\n",
    "            if f\"{factor} -> Market Returns\" in value:\n",
    "                matrix[i, j] = 1  # Causal\n",
    "            elif \"Market Returns ->\" in value:\n",
    "                matrix[i, j] = -1  # Reverse\n",
    "            elif \"Inconclusive\" in value:\n",
    "                matrix[i, j] = 0.5  # Inconclusive\n",
    "            else:\n",
    "                matrix[i, j] = 0  # No effect\n",
    "    \n",
    "    # Heatmap\n",
    "    im = ax.imshow(matrix, cmap='RdYlGn', aspect='auto', vmin=-1, vmax=1)\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(methods)))\n",
    "    ax.set_yticks(np.arange(len(factors_list)))\n",
    "    ax.set_xticklabels(methods)\n",
    "    ax.set_yticklabels(factors_list)\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Causal Direction', rotation=270, labelpad=15)\n",
    "    \n",
    "    # Annotations\n",
    "    for i in range(len(factors_list)):\n",
    "        for j in range(len(methods)):\n",
    "            if matrix[i, j] == 1:\n",
    "                text = \"→\"\n",
    "            elif matrix[i, j] == -1:\n",
    "                text = \"←\"\n",
    "            elif matrix[i, j] == 0.5:\n",
    "                text = \"?\"\n",
    "            else:\n",
    "                text = \"×\"\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", color=\"black\", fontsize=14)\n",
    "    \n",
    "    ax.set_title(\"Causal Discovery Results\\n(→: Factor causes Excess Returns, ←: Reverse, ?: Inconclusive, ×: No effect)\")\n",
    "    plt.tight_layout()\n",
    "    save_fig(plt.gcf(), 'causal_discovery_summary_real')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac19d9-bfcc-420b-bd6a-2f613ea03dbe",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## 8. Summary\n",
    "\n",
    " Analysis of real Fama-French data using three causal discovery methods:\n",
    "\n",
    " - **PC Algorithm**: Discovers overall causal structure between factors and excess returns\n",
    " - **ANM**: Tests pairwise causal directions with non-linear relationships\n",
    " - **DIVOT**: Uses optimal transport for distributional causal discovery\n",
    "\n",
    " Results show which factors have strongest causal relationships with excess returns based on agreement across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8714dd4-4c71-4f18-bb32-ac000944e1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis complete. Check 'Graphs/Real' directory for visualizations.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "print(\"\\nAnalysis complete. Check 'Graphs/Real' directory for visualizations.\") "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
